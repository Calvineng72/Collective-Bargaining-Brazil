{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_id</th>\n",
       "      <th>acordo</th>\n",
       "      <th>extrato</th>\n",
       "      <th>validity</th>\n",
       "      <th>name</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Minimum wage</td>\n",
       "      <td>Wage adjustment</td>\n",
       "      <td>Ficará garantido ao empregado motorista o valo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Salary payment - means and timeframes</td>\n",
       "      <td>Wage payment</td>\n",
       "      <td>Para as funções de motorista de carreta, bi-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Salary deductions</td>\n",
       "      <td>Wage adjustment</td>\n",
       "      <td>Qualquer multa por excesso de velocidade, por ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Food assistance</td>\n",
       "      <td>Assistances</td>\n",
       "      <td>Os empregados motoristas externos receberão me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Food assistance</td>\n",
       "      <td>Assistances</td>\n",
       "      <td>Levando-se em conta a crise econômica e a redu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contract_id  acordo  extrato  validity  \\\n",
       "0  2009_055971       1        1         1   \n",
       "1  2009_055971       1        1         1   \n",
       "2  2009_055971       1        1         1   \n",
       "3  2009_055971       1        1         1   \n",
       "4  2009_055971       1        1         1   \n",
       "\n",
       "                                    name         subgroup  \\\n",
       "0                           Minimum wage  Wage adjustment   \n",
       "1  Salary payment - means and timeframes     Wage payment   \n",
       "2                      Salary deductions  Wage adjustment   \n",
       "3                        Food assistance      Assistances   \n",
       "4                        Food assistance      Assistances   \n",
       "\n",
       "                                                text  \n",
       "0  Ficará garantido ao empregado motorista o valo...  \n",
       "1  Para as funções de motorista de carreta, bi-tr...  \n",
       "2  Qualquer multa por excesso de velocidade, por ...  \n",
       "3  Os empregados motoristas externos receberão me...  \n",
       "4  Levando-se em conta a crise econômica e a redu...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reads file as csv\n",
    "df = pd.read_csv('clause_data/contract_clauses.csv', sep=\"|\")\n",
    "\n",
    "# keeps only valid ACTs\n",
    "df = df.loc[(df['acordo'] == 1)&(df['extrato'] == 1)&(df['validity'] == 1)]\n",
    "df = df.dropna()\n",
    "\n",
    "# reindexes the dataframe with the default integer index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(len(df.index))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/calvineng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/calvineng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# adds custom stop words\n",
    "custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x',\n",
    "                     'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx', 'xxi',\n",
    "                     'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxix', 'xxx']\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "# stemmer\n",
    "stemmer = SnowballStemmer('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CBApreprocess(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, preprocess=True, remove_punctuation=True,\n",
    "                 replace_numbers=True, remove_stopwords=True, stemming=True):\n",
    "        self.preprocess = preprocess\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stemming = stemming\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]|º', '', text)\n",
    "        if self.replace_numbers:\n",
    "            text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "        if self.remove_stopwords:\n",
    "            words = text.split()\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "            text = ' '.join(words)\n",
    "        if self.stemming and stemmer is not None:\n",
    "            words = text.split()\n",
    "            words = [stemmer.stem(word) for word in words]\n",
    "            text = ' '.join(words)\n",
    "        return text.lower()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = [\n",
    "            self.preprocess_text(cba)\n",
    "            for cba in X\n",
    "        ]\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class CBAToTFIDFTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_tfidf = self.vectorizer.transform(X)\n",
    "        return X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# lemmatizer\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# stop words\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# adds custom stop words\n",
    "custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x',\n",
    "                     'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx', 'xxi',\n",
    "                     'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxix', 'xxx']\n",
    "stop_words.update(custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAPreprocessLemmatize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, preprocess=True, remove_punctuation=True,\n",
    "                 replace_numbers=True, remove_stopwords=True, lemmatize=True):\n",
    "        self.preprocess = preprocess\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]|º', '', text)\n",
    "        if self.replace_numbers:\n",
    "            text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "        if self.remove_stopwords:\n",
    "            words = text.split()\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "            text = ' '.join(words)\n",
    "        if self.lemmatize:\n",
    "            doc = nlp(text)\n",
    "            words = [token.lemma_ for token in doc]\n",
    "            text = ' '.join(words)\n",
    "        return text.lower()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = [\n",
    "            self.preprocess_text(cba)\n",
    "            for cba in X\n",
    "        ]\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for 'Assistances': ['acord', 'fic', 'pagament', 'dependent', 'benefíci', 'conced', 'rea', 'val', 'auxíli', 'aliment', 'pag', 'cas', 'transport', 'ser', 'fornec', 'salári', 'trabalh', 'empres', 'empreg', 'numb']\n",
      "Top 20 words for 'Bonuses': ['ano', 'receb', 'caix', 'promoçã', 'funçã', 'adiant', 'gratific', 'cinquent', 'pagament', 'parcel', 'dia', 'dias', 'pag', 'trabalh', 'ser', 'fér', 'empres', 'salári', 'empreg', 'numb']\n",
      "Top 20 words for 'Contract types': ['estagiári', 'acord', 'lei', 'aprendizag', 'períod', 'mesm', 'funçã', 'superior', 'estági', 'praz', 'ser', 'salári', 'dias', 'aprendiz', 'trabalh', 'empres', 'experient', 'empreg', 'contrat', 'numb']\n",
      "Top 20 words for 'Employment protections': ['just', 'provisór', 'afast', 'anos', 'gestant', 'salári', 'períod', 'aposentador', 'milit', 'praz', 'assegur', 'fic', 'empres', 'trabalh', 'estabil', 'garant', 'servic', 'dias', 'numb', 'empreg']\n",
      "Top 20 words for 'General provisions': ['dirim', 'descumpr', 'justic', 'salári', 'estabelec', 'cas', 'aplic', 'ser', 'sindicat', 'part', 'fic', 'mult', 'empres', 'cláusul', 'colet', 'present', 'empreg', 'acord', 'trabalh', 'numb']\n",
      "Top 20 words for 'Hiring': ['servic', 'admissã', 'fic', 'praz', 'exerc', 'dias', 'cas', 'anot', 'acord', 'admit', 'ser', 'mesm', 'salári', 'experient', 'funçã', 'trabalh', 'empres', 'contrat', 'numb', 'empreg']\n",
      "Top 20 words for 'Holidays': ['trint', 'antecedent', 'trabalh', 'comunic', 'compens', 'dia', 'individu', 'sáb', 'coincid', 'doming', 'goz', 'feri', 'períod', 'empres', 'colet', 'iníci', 'empreg', 'dias', 'fér', 'numb']\n",
      "Top 20 words for 'Leave': ['trabalh', 'ano', 'fic', 'empres', 'idad', 'adot', 'gestant', 'remuner', 'judicial', 'adoçã', 'cas', 'ser', 'guard', 'conced', 'anos', 'crianc', 'licenc', 'dias', 'empreg', 'numb']\n",
      "Top 20 words for 'Other income': ['rea', 'cas', 'present', 'lucr', 'vid', 'aposentador', 'anos', 'servic', 'pag', 'pagament', 'salári', 'ser', 'result', 'acord', 'particip', 'segur', 'trabalh', 'empres', 'empreg', 'numb']\n",
      "Top 20 words for 'Other wages': ['correspondent', 'comission', 'descans', 'dias', 'can', 'cort', 'lei', 'feri', 'hor', 'empres', 'repous', 'durant', 'aprendiz', 'empreg', 'semanal', 'ser', 'remuner', 'salári', 'trabalh', 'numb']\n",
      "Top 20 words for 'Other: adjustments, payments, wages': ['dias', 'salarial', 'substituiçã', 'comprov', 'adiant', 'remuner', 'efetu', 'dia', 'funçã', 'fornec', 'descont', 'hor', 'pag', 'ser', 'trabalh', 'pagament', 'empres', 'salári', 'empreg', 'numb']\n",
      "Top 20 words for 'Other: employment contract': ['rescisã', 'cas', 'ctps', 'fic', 'caus', 'just', 'mesm', 'funçã', 'anot', 'fornec', 'ser', 'acord', 'praz', 'dispens', 'salári', 'contrat', 'trabalh', 'empres', 'empreg', 'numb']\n",
      "Top 20 words for 'Other: holidays and leave': ['assegur', 'colet', 'iníci', 'incis', 'trabalh', 'fic', 'ser', 'remuner', 'empres', 'úte', 'compens', 'feri', 'dia', 'art', 'licenc', 'casament', 'fér', 'empreg', 'dias', 'numb']\n",
      "Top 20 words for 'Pay': ['cinquent', 'extraordinár', 'cem', 'trint', 'salári', 'empres', 'dia', 'pag', 'acréscim', 'extras', 'serã', 'normal', 'ser', 'empreg', 'remuner', 'noturn', 'adicional', 'trabalh', 'numb', 'hor']\n",
      "Top 20 words for 'Prevention': ['cip', 'serã', 'condiçõ', 'mant', 'fic', 'exam', 'médic', 'acident', 'segur', 'proteçã', 'equip', 'uso', 'exig', 'gratuit', 'uniform', 'fornec', 'numb', 'trabalh', 'empres', 'empreg']\n",
      "Top 20 words for 'Protections': ['afast', 'reconhec', 'fornec', 'cas', 'convêni', 'profissional', 'serã', 'servic', 'dias', 'emit', 'aceit', 'sindicat', 'acident', 'odontológ', 'trabalh', 'empreg', 'empres', 'numb', 'atest', 'médic']\n",
      "Top 20 words for 'Separations': ['servic', 'homolog', 'motiv', 'fic', 'ser', 'contrat', 'rescisã', 'cas', 'anos', 'escrit', 'just', 'caus', 'dias', 'prévi', 'dispens', 'empres', 'avis', 'trabalh', 'empreg', 'numb']\n",
      "Top 20 words for 'Staffing rules': ['contrat', 'profissional', 'fic', 'hor', 'acord', 'períod', 'dev', 'ser', 'funçã', 'cas', 'dias', 'carg', 'curs', 'solicit', 'fornec', 'salári', 'trabalh', 'numb', 'empres', 'empreg']\n",
      "Top 20 words for 'Union organization': ['folh', 'dias', 'ser', 'assoc', 'entidad', 'pagament', 'dia', 'sindical', 'salári', 'acord', 'mensal', 'profissional', 'trabalh', 'contribuiçã', 'recolh', 'empres', 'sindicat', 'empreg', 'descont', 'numb']\n",
      "Top 20 words for 'Union-firm relations': ['represent', 'colet', 'diretor', 'dirigent', 'afix', 'sindic', 'categor', 'sindical', 'comunic', 'entidad', 'dias', 'quadr', 'profissional', 'acord', 'avis', 'trabalh', 'empreg', 'sindicat', 'numb', 'empres']\n",
      "Top 20 words for 'Wage adjustment': ['períod', 'aument', 'mai', 'aplic', 'colet', 'descont', 'conced', 'serã', 'fic', 'ser', 'pis', 'acord', 'empres', 'rea', 'trabalh', 'salarial', 'reajust', 'empreg', 'salári', 'numb']\n",
      "Top 20 words for 'Wage payment': ['subsequent', 'remuner', 'cont', 'chequ', 'discrimin', 'comprov', 'fornec', 'ser', 'trabalh', 'descont', 'adiant', 'pag', 'útil', 'efetu', 'empres', 'empreg', 'dia', 'salári', 'pagament', 'numb']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# creates Pipeline to preprocess and calculate TF-IDF\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"cba_preprocess\", CBApreprocess()),\n",
    "    (\"cba_to_tfidf\", CBAToTFIDFTransformer())\n",
    "])\n",
    "\n",
    "# number of words to output and groupings\n",
    "groups = df.groupby('subgroup')\n",
    "n_top_words = 20\n",
    "\n",
    "for name, group in groups:\n",
    "    data_texts = group['text'].tolist()\n",
    "    data_tfidf = preprocess_pipeline.fit_transform(data_texts)\n",
    "    mean_tfidf = data_tfidf.mean(axis=0)\n",
    "    top_indices = np.argsort(mean_tfidf)[-n_top_words:]\n",
    "    vocab = preprocess_pipeline.named_steps['cba_to_tfidf'].vectorizer.get_feature_names_out()\n",
    "    top_words = [vocab[idx] for idx in top_indices]\n",
    "    print(f\"Top {n_top_words} words for '{name}': {top_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
