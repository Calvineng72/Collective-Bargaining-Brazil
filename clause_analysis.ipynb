{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from itertools import dropwhile, takewhile\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the output directory\n",
    "cba_path = os.path.join(\".\", \"clause_analysis\")\n",
    "if not os.path.isdir(cba_path):\n",
    "    os.mkdir(cba_path)\n",
    "\n",
    "# sets the input directory\n",
    "file_path = os.getcwd() + '/cbas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_groups = pd.read_csv('clause_groups.csv', index_col='Clause Group')\n",
    "translation_dict = clause_groups['Translation'].to_dict()\n",
    "themes = list(map(str, clause_groups['Theme'].unique()))\n",
    "theme_dict = clause_groups['Theme'].to_dict()\n",
    "\n",
    "def extract_clauses(file_path, clause_type):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        # removes white space from the ends of lines\n",
    "        lines = (line.strip() for line in f)  \n",
    "\n",
    "        # retrieves the validity\n",
    "        validity_start_flag = dropwhile(lambda line: '<STARTofVALIDITY>' not in line, lines)\n",
    "        next(validity_start_flag,\"\")\n",
    "        validity_end_flag = takewhile(lambda line: '<ENDofVALIDITY>' not in line, validity_start_flag)\n",
    "        validity = '\\n'.join(validity_end_flag).strip()\n",
    "    \n",
    "        # extracts the types of clauses present\n",
    "        clause_flag_start = dropwhile(lambda line: '<STARTofCLAUSES>' not in line, lines)\n",
    "        next(clause_flag_start,\"\")\n",
    "        clause_flag_end = takewhile(lambda line: '<ENDofCLAUSES>' not in line, clause_flag_start)\n",
    "        themes = []\n",
    "        titles = []\n",
    "        for line in clause_flag_end:\n",
    "            if not line: \n",
    "                continue  \n",
    "            title = line.split('|')[0]\n",
    "            translation = translation_dict[title]\n",
    "            titles.append(translation)\n",
    "            theme = theme_dict[title]\n",
    "            themes.append(theme)\n",
    "\n",
    "        # extracts the text of clauses\n",
    "        text_flag_start = dropwhile(lambda line: '<STARTofTEXT>' not in line, lines)\n",
    "        next(text_flag_start, \"\")\n",
    "        texts = []\n",
    "        text = []\n",
    "        for line in text_flag_start:\n",
    "            if '|' in line: \n",
    "                text.append(line.split('|')[0])\n",
    "                texts.append((' ').join(text))\n",
    "                text = []\n",
    "            else:\n",
    "                text.append(line)\n",
    "        if text:\n",
    "            texts.append((' ').join(text))\n",
    "\n",
    "        # retains clauses of proper type\n",
    "        indices_of_type = [i for i, theme in enumerate(themes) if theme == clause_type]\n",
    "        titles_of_type = [titles[i] for i in indices_of_type]\n",
    "        texts_of_type = [texts[i] for i in indices_of_type]\n",
    "\n",
    "        return validity, titles_of_type, texts_of_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_all(file_path_x, files_x, clause_type):\n",
    "    # only considers files with start dates 2008-2017\n",
    "    if files_x[0:4].isdigit() and 2008 <= int(files_x[0:4]) <= 2017:\n",
    "        # contract identifier\n",
    "        contract_id = [files_x[-15:-4]]\n",
    "        if len(files_x[-15:-4]) != 11:\n",
    "            pass\n",
    "        validity, titles, texts = extract_clauses(os.path.join(file_path_x, files_x), clause_type)\n",
    "        # saves info for contract as a single new line\n",
    "        pairs = [(contract_id + [validity, title, text]) for title, text in zip(titles, texts)]\n",
    "        with io.open(path_txt, 'a', encoding='utf8') as f:\n",
    "            for pair in pairs:\n",
    "                pair_line = '|'.join(str(x) for x in pair)\n",
    "                f.write(pair_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme in themes: \n",
    "    file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "\n",
    "    # rewrites output file\n",
    "    path_txt = os.path.join(cba_path, f\"{file_name}_text.csv\")\n",
    "    with io.open(path_txt,'w',encoding='utf8') as f:\n",
    "        header = 'contract_id|validity|title|text'\n",
    "        f.write(header + '\\n')\n",
    "\n",
    "    # loops over each contract\n",
    "    [output_all(file_path, files, theme) for files in os.listdir(file_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: Wages\n",
      "Best value of C: 100\n",
      "Test accuracy: 0.6285714285714286\n",
      "Top indicative features: ['ocorrer', 'parágrafo', 'comprovantes', 'idade', 'ão', 'quarenta', 'trinto', 'teto', 'média', 'salario', 'empregador', 'admissão', 'feira', 'curso', 'cumprimento', 'assuma', 'salvo', 'motorista', 'cláusula', 'abono']\n",
      "Theme: Health\n",
      "Best value of C: 0.001\n",
      "Test accuracy: 0.625\n",
      "Top indicative features: ['útil', 'documento', 'doenças', 'drogaria', 'duração', 'duzento', 'econômica', 'efeito', 'efetivo', 'efetuar', 'eficácia', 'elaborar', 'eletrônico', 'em', 'email', 'emergência', 'emissão', 'emitente', 'emitido', 'emitir']\n",
      "Theme: Union\n",
      "Best value of C: 10\n",
      "Test accuracy: 0.6153846153846154\n",
      "Top indicative features: ['empregador', 'escrito', 'titular', 'cidade', 'disposto', 'ão', 'envolver', 'salarial', 'surgir', 'parágrafo', 'haver', 'data', 'sindical', 'recolhir', 'qualquer', 'imposto', 'confederativa', 'disposição', 'pagamento', 'demissão']\n",
      "Theme: Safety / Injury / Disability\n",
      "Best value of C: 0.001\n",
      "Test accuracy: 0.6666666666666666\n",
      "Top indicative features: ['útil', 'decorrir', 'encaminhar', 'encaminhamento', 'empresa', 'emprego', 'empregar', 'empregados', 'empregadoro', 'empregadora', 'empregador', 'empregado', 'emissão', 'emergência', 'eliminar', 'eletivo', 'elencado', 'eleições', 'eleição', 'eleitos']\n",
      "Theme: Work Adaptation / Training\n",
      "Best value of C: 0.001\n",
      "Test accuracy: 0.75\n",
      "Top indicative features: ['único', 'efetivar', 'dispensar', 'dispenso', 'disposição', 'divulgar', 'dobro', 'doença', 'dsr', 'durante', 'educação', 'efetivação', 'disciplinar', 'efetivo', 'empregada', 'empregado', 'empregador', 'empregar', 'emprego', 'empresa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: Work Time\n",
      "Best value of C: 0.001\n",
      "Test accuracy: 0.7428571428571429\n",
      "Top indicative features: ['útil', 'domingo', 'doméstico', 'drástico', 'dsr', 'dupla', 'durante', 'duração', 'duzento', 'débito', 'décimo', 'econômica', 'econômico', 'educacional', 'educação', 'efeito', 'efetivamente', 'efetivar', 'efetivação', 'efetiver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: Incentives\n",
      "Best value of C: 100\n",
      "Test accuracy: 0.8\n",
      "Top indicative features: ['mesmo', 'certidão', 'ela', 'dezembro', 'dia', 'diferença', 'direito', 'disponivel', 'dobro', 'documentos', 'durante', 'efeito', 'único', 'devidamente', 'elegibilidade', 'empregada', 'empregado', 'empregador', 'empregadora', 'empregar']\n",
      "Theme: Food / Education / Housing\n",
      "Best value of C: 0.001\n",
      "Test accuracy: 0.75\n",
      "Top indicative features: ['útil', 'empregadora', 'empregado', 'empregada', 'emitir', 'eletrônico', 'ele', 'efetuar', 'efetivo', 'efetivamente', 'efeito', 'educação', 'edição', 'décimo', 'décima', 'duzento', 'duração', 'durante', 'dou', 'dotar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: Contract Agreement\n",
      "Best value of C: 0.001\n",
      "Test accuracy: 0.7777777777777778\n",
      "Top indicative features: ['único', 'escrito', 'empregador', 'empregados', 'empregar', 'emprego', 'empresa', 'empresas', 'ensejar', 'entidade', 'envolver', 'equipamento', 'equivalente', 'esclarecimento', 'especificado', 'item', 'especificar', 'específico', 'estabelecem', 'estabelecer']\n",
      "Theme: Retirement\n",
      "Best value of C: 1000\n",
      "Test accuracy: 0.25\n",
      "Top indicative features: ['garantia', 'estabilidade', 'trabalhador', 'permanente', 'rescisão', 'normativo', 'ão', 'época', 'parágrafo', 'invalidez', 'documento', 'devir', 'documentação', 'efetuar', 'durante', 'dispensar', 'dispensa', 'discussão', 'efeito', 'direito']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: Work Environment / Harassment\n",
      "Best value of C: 100\n",
      "Test accuracy: 0.6\n",
      "Top indicative features: ['trabalhador', 'condição', 'exigir', 'pessoal', 'mesmo', 'ficar', 'feminino', 'empregador', 'potável', 'ônus', 'utilizar', 'exigência', 'exclusivo', 'exigido', 'exiger', 'filtrar', 'exemplarmente', 'formulário', 'expor', 'estabelecimento']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: Family\n",
      "Best value of C: 10\n",
      "Test accuracy: 0.4444444444444444\n",
      "Top indicative features: ['correspondente', 'hipótese', 'conforme', 'conceder', 'hora', 'seguro', 'ano', 'pedir', 'setenta', 'diário', 'divulgação', 'disponibilizar', 'doação', 'distrito', 'dobro', 'disposto', 'documentalista', 'disponível', 'útil', 'dispensar']\n",
      "Theme: Dismissals / Transfers\n",
      "Best value of C: 0.001\n",
      "Test accuracy: 0.5833333333333334\n",
      "Top indicative features: ['útil', 'dissolução', 'disposição', 'dispositivo', 'dispor', 'dispenso', 'dispensar', 'dispensado', 'dispensa', 'discussão', 'discriminar', 'dirimir', 'diretoria', 'diretamente', 'direito', 'dimensional', 'dificuldade', 'diferença', 'dia', 'disposto']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "14 fits failed out of a total of 14.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1528, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\", line 1143, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'carimbo'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/calvineng/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'carimbo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/lfh0pr6s18l0t4hm_q6nnb_m0000gn/T/ipykernel_50776/386571734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1526\u001b[0m                     \u001b[0;34m\" = {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                 )\n\u001b[0;32m-> 1528\u001b[0;31m             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[1;32m   1529\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                 \u001b[0;34m\"This solver needs samples of at least 2 classes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'carimbo'"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "for theme in themes:\n",
    "    # reads file as csv\n",
    "    file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "    df = pd.read_csv(f'clause_analysis/{file_name}_text.csv', sep='|')\n",
    "\n",
    "    # cleans the text\n",
    "    df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # calculatues TFIDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df['clean_text'])\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "    # splits the data for training and testing\n",
    "    y = (df['validity']=='carimbo').to_numpy()\n",
    "    y = df['validity']\n",
    "    X = X.toarray()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # trains a logistic regression model with L1 penalty and liblinear solver\n",
    "    logreg = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "    # finds the best value of C \n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    grid = GridSearchCV(logreg, param_grid, cv=2)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # prints the best value of C and the test accuracy\n",
    "    print(f\"Theme: {theme}\")\n",
    "    print(f\"Best value of C: {best_model.C}\")\n",
    "    print(f\"Test accuracy: {best_model.score(X_test, y_test)}\")\n",
    "\n",
    "    # gets the top indicative features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coefs = best_model.coef_[0]\n",
    "    top_indicative_features = [feature_names[i] for i in coefs.argsort()[-20:][::-1]]\n",
    "    print(\"Top indicative features:\", top_indicative_features)\n",
    "\n",
    "    # saves to csv\n",
    "    df.to_csv(f'clause_analysis/{file_name}_text.csv', sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
