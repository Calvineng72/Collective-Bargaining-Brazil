{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from itertools import dropwhile, takewhile\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the output directory\n",
    "cba_path = os.path.join(\".\", \"clause_analysis\")\n",
    "if not os.path.isdir(cba_path):\n",
    "    os.mkdir(cba_path)\n",
    "\n",
    "# sets the input directory\n",
    "file_path = os.getcwd() + '/cbas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_groups = pd.read_csv('clause_groups.csv', index_col='Clause Group')\n",
    "translation_dict = clause_groups['Translation'].to_dict()\n",
    "themes = list(map(str, clause_groups['Theme'].unique()))\n",
    "theme_dict = clause_groups['Theme'].to_dict()\n",
    "\n",
    "def extract_clauses(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        # removes white space from the ends of lines\n",
    "        lines = (line.strip() for line in f)  \n",
    "\n",
    "        # retrieves the validity\n",
    "        validity_start_flag = dropwhile(lambda line: '<STARTofVALIDITY>' not in line, lines)\n",
    "        next(validity_start_flag,\"\")\n",
    "        validity_end_flag = takewhile(lambda line: '<ENDofVALIDITY>' not in line, validity_start_flag)\n",
    "        validity = '\\n'.join(validity_end_flag).strip()\n",
    "    \n",
    "        # extracts the types of clauses present\n",
    "        clause_flag_start = dropwhile(lambda line: '<STARTofCLAUSES>' not in line, lines)\n",
    "        next(clause_flag_start,\"\")\n",
    "        clause_flag_end = takewhile(lambda line: '<ENDofCLAUSES>' not in line, clause_flag_start)\n",
    "        themes = []\n",
    "        titles = []\n",
    "        for line in clause_flag_end:\n",
    "            if not line: \n",
    "                continue  \n",
    "            title = line.split('|')[0]\n",
    "            translation = translation_dict[title]\n",
    "            titles.append(translation)\n",
    "            theme = theme_dict[title]\n",
    "            themes.append(theme)\n",
    "\n",
    "        # extracts the text of clauses\n",
    "        text_flag_start = dropwhile(lambda line: '<STARTofTEXT>' not in line, lines)\n",
    "        next(text_flag_start, \"\")\n",
    "        texts = []\n",
    "        text = []\n",
    "        for line in text_flag_start:\n",
    "            if '|' in line: \n",
    "                text.append(line.split('|')[0])\n",
    "                texts.append((' ').join(text))\n",
    "                text = []\n",
    "            else:\n",
    "                text.append(line)\n",
    "        if text:\n",
    "            texts.append((' ').join(text))\n",
    "\n",
    "        return validity, themes, titles, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_all(file_path_x, files_x):\n",
    "    # only considers files with start dates 2008-2017\n",
    "    if files_x[0:4].isdigit() and 2008 <= int(files_x[0:4]) <= 2017:\n",
    "        # contract identifier\n",
    "        contract_id = [files_x[-15:-4]]\n",
    "        if len(files_x[-15:-4]) != 11:\n",
    "            pass\n",
    "        validity, themes, titles, texts = extract_clauses(os.path.join(file_path_x, files_x))\n",
    "        # saves info for contract as a single new line\n",
    "        pairs = [(contract_id + [validity, theme, title, text]) for theme, title, text in zip(themes, titles, texts)]\n",
    "        with io.open(path_txt, 'a', encoding='utf8') as f:\n",
    "            for pair in pairs:\n",
    "                pair_line = '|'.join(str(x) for x in pair)\n",
    "                f.write(pair_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through file  2016_09_01__2016_082054.txt\n",
      "Looping through file  2011_11_01__2012_002993.txt\n",
      "Looping through file  2014_01_01__2014_081501.txt\n",
      "Looping through file  2017_12_01__2017_084835.txt\n",
      "Looping through file  2017_12_01__2017_084809.txt\n",
      "Looping through file  2013_11_15__2013_055346.txt\n",
      "Looping through file  2009_01_01__2009_016497.txt\n",
      "Looping through file  2015_06_16__2015_060659.txt\n",
      "Looping through file  2018_05_01__2018_044118.txt\n",
      "Looping through file  2012_05_01__2012_042451.txt\n",
      "Looping through file  2011_11_01__2012_002943.txt\n",
      "Looping through file  2016_09_01__2016_082084.txt\n",
      "Looping through file  2013_11_14__2014_009174.txt\n",
      "Looping through file  2009_01_01__2009_016731.txt\n",
      "Looping through file  2015_05_01__2015_043073.txt\n",
      "Looping through file  2011_11_01__2012_003082.txt\n",
      "Looping through file  2015_12_16__2015_084042.txt\n",
      "Looping through file  2017_12_01__2017_084934.txt\n",
      "Looping through file  2013_06_01__2013_073146.txt\n",
      "Looping through file  2017_03_01__2017_039221.txt\n",
      "Looping through file  2017_03_01__2017_039341.txt\n",
      "Looping through file  2015_06_16__2015_069545.txt\n",
      "Looping through file  2013_11_15__2013_069396.txt\n",
      "Looping through file  2009_01_01__2009_016579.txt\n",
      "Looping through file  2014_01_01__2014_038109.txt\n",
      "Looping through file  2013_06_01__2013_073403.txt\n",
      "Looping through file  2010_04_01__2010_074416.txt\n",
      "Looping through file  2015_01_01__2015_065035.txt\n",
      "Looping through file  2017_03_01__2017_039364.txt\n",
      "Looping through file  2012_05_01__2012_032527.txt\n",
      "Looping through file  2009_01_01__2009_034089.txt\n",
      "Looping through file  2013_11_15__2013_068336.txt\n",
      "Looping through file  2018_05_01__2018_044198.txt\n",
      "Looping through file  2014_01_01__2014_081419.txt\n",
      "Looping through file  2013_06_01__2013_073410.txt\n",
      "Looping through file  2009_01_01__2009_016389.txt\n",
      "Looping through file  2010_04_01__2010_074884.txt\n",
      "Looping through file  2014_01_01__2014_038173.txt\n",
      "Looping through file  2012_05_01__2012_042462.txt\n",
      "Looping through file  2012_05_01__2012_042489.txt\n",
      "Looping through file  2016_09_01__2016_082062.txt\n",
      "Looping through file  2010_04_01__2010_075192.txt\n",
      "Looping through file  2015_05_01__2015_043120.txt\n",
      "Looping through file  2014_01_01__2014_081490.txt\n",
      "Looping through file  2015_06_16__2015_075846.txt\n",
      "Looping through file  2012_05_01__2012_032551.txt\n",
      "Looping through file  2010_04_01__2010_074659.txt\n",
      "Looping through file  2015_05_01__2015_043084.txt\n",
      "Looping through file  2015_12_16__2015_083965.txt\n",
      "Looping through file  2015_06_17__2015_031484.txt\n"
     ]
    }
   ],
   "source": [
    "# rewrites output file\n",
    "path_txt = os.path.join(cba_path, \"data.csv\")\n",
    "with io.open(path_txt,'w',encoding='utf8') as f:\n",
    "    header = 'contract_id|validity|theme|title|text'\n",
    "    f.write(header + '\\n')\n",
    "\n",
    "# loops over each contract\n",
    "for idx, files in enumerate(os.listdir(file_path)):\n",
    "    print(\"Looping through file \", files)\n",
    "    output_all(file_path, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_id</th>\n",
       "      <th>validity</th>\n",
       "      <th>theme</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>carimbo</th>\n",
       "      <th>semvalorlegal</th>\n",
       "      <th>carimbo</th>\n",
       "      <th>semvalorlegal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Minimum Wage</td>\n",
       "      <td>- SALÁRIOS NORMATIVOS  Fica assegurado, para o...</td>\n",
       "      <td>salários normativos ficar assegurar empregado ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Adjustments / Corrections</td>\n",
       "      <td>Os salários dos empregados (as) serão corrigi...</td>\n",
       "      <td>salário empregado ser corrigir percentual vírg...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Adjustments / Corrections</td>\n",
       "      <td>COMPENSAÇÕES  Serão antes COMPENSADOS DA APLIC...</td>\n",
       "      <td>compensações serão compensados aplicação reaju...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Adjustments / Corrections</td>\n",
       "      <td>- ADMISSÃO APÓS DATA-BASE  O reajuste salarial...</td>\n",
       "      <td>admissão reajuste salarial empregado admitir o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Payment</td>\n",
       "      <td>- PAGAMENTO DE SALÁRIOS  A) A empresa deverá p...</td>\n",
       "      <td>pagamento salários empresa proporcionar empreg...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contract_id validity  theme                             title  \\\n",
       "0  2016_082054  carimbo  Wages                      Minimum Wage   \n",
       "1  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
       "2  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
       "3  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
       "4  2016_082054  carimbo  Wages                    Salary Payment   \n",
       "\n",
       "                                                text  \\\n",
       "0  - SALÁRIOS NORMATIVOS  Fica assegurado, para o...   \n",
       "1   Os salários dos empregados (as) serão corrigi...   \n",
       "2  COMPENSAÇÕES  Serão antes COMPENSADOS DA APLIC...   \n",
       "3  - ADMISSÃO APÓS DATA-BASE  O reajuste salarial...   \n",
       "4  - PAGAMENTO DE SALÁRIOS  A) A empresa deverá p...   \n",
       "\n",
       "                                          clean_text  carimbo  semvalorlegal  \\\n",
       "0  salários normativos ficar assegurar empregado ...        1              0   \n",
       "1  salário empregado ser corrigir percentual vírg...        1              0   \n",
       "2  compensações serão compensados aplicação reaju...        1              0   \n",
       "3  admissão reajuste salarial empregado admitir o...        1              0   \n",
       "4  pagamento salários empresa proporcionar empreg...        1              0   \n",
       "\n",
       "   carimbo  semvalorlegal  \n",
       "0        1              0  \n",
       "1        1              0  \n",
       "2        1              0  \n",
       "3        1              0  \n",
       "4        1              0  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization package\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# adds custom stop words\n",
    "custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']\n",
    "for word in custom_stop_words:\n",
    "    stop_words.add(word)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# reads data into dataframe\n",
    "df = pd.read_csv(f'clause_analysis/data.csv', sep='|')\n",
    "\n",
    "# cleans the text\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# creates dummies for validity\n",
    "validity_dummies = pd.get_dummies(df['validity'])\n",
    "df = pd.concat([df, validity_dummies], axis=1)\n",
    "\n",
    "# saves csv file for analysis\n",
    "df.to_csv('clause_analysis/data.csv', sep='|')\n",
    "\n",
    "# displays head of csv\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            validity  theme                             title  \\\n",
      "contract_id                                                     \n",
      "2016_082054  carimbo  Wages                      Minimum Wage   \n",
      "2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
      "2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
      "2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
      "2016_082054  carimbo  Wages                    Salary Payment   \n",
      "\n",
      "                                                          text  \\\n",
      "contract_id                                                      \n",
      "2016_082054  - SALÁRIOS NORMATIVOS  Fica assegurado, para o...   \n",
      "2016_082054   Os salários dos empregados (as) serão corrigi...   \n",
      "2016_082054  COMPENSAÇÕES  Serão antes COMPENSADOS DA APLIC...   \n",
      "2016_082054  - ADMISSÃO APÓS DATA-BASE  O reajuste salarial...   \n",
      "2016_082054  - PAGAMENTO DE SALÁRIOS  A) A empresa deverá p...   \n",
      "\n",
      "                                                    clean_text  carimbo  \\\n",
      "contract_id                                                               \n",
      "2016_082054  salários normativos ficar assegurar empregado ...        1   \n",
      "2016_082054  salário empregado ser corrigir percentual vírg...        1   \n",
      "2016_082054  compensações serão compensados aplicação reaju...        1   \n",
      "2016_082054  admissão reajuste salarial empregado admitir o...        1   \n",
      "2016_082054  pagamento salários empresa proporcionar empreg...        1   \n",
      "\n",
      "             semvalorlegal  \n",
      "contract_id                 \n",
      "2016_082054              0  \n",
      "2016_082054              0  \n",
      "2016_082054              0  \n",
      "2016_082054              0  \n",
      "2016_082054              0  \n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/lfh0pr6s18l0t4hm_q6nnb_m0000gn/T/ipykernel_66745/2317793613.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtfidf_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# splits the data for training and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    589\u001b[0m                     \u001b[0mobj_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m                         \u001b[0mindexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3729\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requires_unique_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "for theme in themes: \n",
    "    # reads data into dataframe\n",
    "    df = pd.read_csv(f'clause_analysis/data.csv', sep='|')\n",
    "\n",
    "    # filters for theme\n",
    "    df = df[df['theme'] == theme]\n",
    "    print(df.head())\n",
    "\n",
    "    # calculatues TFIDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df['clean_text'])\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "    # splits the data for training and testing\n",
    "    y = df[df['theme'] == theme]['carimbo']\n",
    "    X = X.toarray()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=253)\n",
    "\n",
    "    # trains a logistic regression model with L1 penalty and liblinear solver\n",
    "    logreg = LogisticRegression(penalty='l1', solver='liblinear', random_state=253)\n",
    "\n",
    "    # finds the best value of C \n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    grid = GridSearchCV(logreg, param_grid, cv=2)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # prints the best value of C and the test accuracy\n",
    "    print(f\"Theme: {theme}\")\n",
    "    print(f\"Best value of C: {best_model.C}\")\n",
    "    print(f\"Test accuracy: {best_model.score(X_test, y_test)}\")\n",
    "\n",
    "    # gets the top indicative features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coefs = best_model.coef_[0]\n",
    "    top_indicative_features = [feature_names[i] for i in coefs.argsort()[-20:][::-1]]\n",
    "    print(\"Top indicative features:\", top_indicative_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
