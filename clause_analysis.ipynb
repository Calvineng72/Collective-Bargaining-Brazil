{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from itertools import dropwhile, takewhile\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the output directory\n",
    "cba_path = os.path.join(\".\", \"clause_analysis\")\n",
    "if not os.path.isdir(cba_path):\n",
    "    os.mkdir(cba_path)\n",
    "\n",
    "# sets the input directory\n",
    "file_path = os.getcwd() + '/cbas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_groups = pd.read_csv('clause_groups.csv', index_col='Clause Group')\n",
    "translation_dict = clause_groups['Translation'].to_dict()\n",
    "themes = list(map(str, clause_groups['Theme'].unique()))\n",
    "theme_dict = clause_groups['Theme'].to_dict()\n",
    "\n",
    "def extract_clauses(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        # removes white space from the ends of lines\n",
    "        lines = (line.strip() for line in f)  \n",
    "\n",
    "        # retrieves the validity\n",
    "        validity_start_flag = dropwhile(lambda line: '<STARTofVALIDITY>' not in line, lines)\n",
    "        next(validity_start_flag,\"\")\n",
    "        validity_end_flag = takewhile(lambda line: '<ENDofVALIDITY>' not in line, validity_start_flag)\n",
    "        validity = '\\n'.join(validity_end_flag).strip()\n",
    "    \n",
    "        # extracts the types of clauses present\n",
    "        clause_flag_start = dropwhile(lambda line: '<STARTofCLAUSES>' not in line, lines)\n",
    "        next(clause_flag_start,\"\")\n",
    "        clause_flag_end = takewhile(lambda line: '<ENDofCLAUSES>' not in line, clause_flag_start)\n",
    "        themes = []\n",
    "        titles = []\n",
    "        for line in clause_flag_end:\n",
    "            if not line: \n",
    "                continue  \n",
    "            title = line.split('|')[0]\n",
    "            translation = translation_dict[title]\n",
    "            titles.append(translation)\n",
    "            theme = theme_dict[title]\n",
    "            themes.append(theme)\n",
    "\n",
    "        # extracts the text of clauses\n",
    "        text_flag_start = dropwhile(lambda line: '<STARTofTEXT>' not in line, lines)\n",
    "        next(text_flag_start, \"\")\n",
    "        texts = []\n",
    "        text = []\n",
    "        for line in text_flag_start:\n",
    "            if '|' in line: \n",
    "                text.append(line.split('|')[0])\n",
    "                texts.append((' ').join(text))\n",
    "                text = []\n",
    "            else:\n",
    "                text.append(line)\n",
    "        if text:\n",
    "            texts.append((' ').join(text))\n",
    "\n",
    "        return validity, themes, titles, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_all(file_path_x, files_x):\n",
    "    # only considers files with start dates 2008-2017\n",
    "    if files_x[0:4].isdigit() and 2008 <= int(files_x[0:4]) <= 2017:\n",
    "        # contract identifier\n",
    "        contract_id = [files_x[-15:-4]]\n",
    "        if len(files_x[-15:-4]) != 11:\n",
    "            pass\n",
    "        validity, themes, titles, texts = extract_clauses(os.path.join(file_path_x, files_x))\n",
    "        # saves info for contract as a single new line\n",
    "        pairs = [(contract_id + [validity, theme, title, text]) for theme, title, text in zip(themes, titles, texts)]\n",
    "        with io.open(path_txt, 'a', encoding='utf8') as f:\n",
    "            for pair in pairs:\n",
    "                pair_line = '|'.join(str(x) for x in pair)\n",
    "                f.write(pair_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through file  2016_09_01__2016_082054.txt\n",
      "Looping through file  2011_11_01__2012_002993.txt\n",
      "Looping through file  2014_01_01__2014_081501.txt\n",
      "Looping through file  2017_12_01__2017_084835.txt\n",
      "Looping through file  2017_12_01__2017_084809.txt\n",
      "Looping through file  2013_11_15__2013_055346.txt\n",
      "Looping through file  2009_01_01__2009_016497.txt\n",
      "Looping through file  2015_06_16__2015_060659.txt\n",
      "Looping through file  2018_05_01__2018_044118.txt\n",
      "Looping through file  2012_05_01__2012_042451.txt\n",
      "Looping through file  2011_11_01__2012_002943.txt\n",
      "Looping through file  2016_09_01__2016_082084.txt\n",
      "Looping through file  2013_11_14__2014_009174.txt\n",
      "Looping through file  2009_01_01__2009_016731.txt\n",
      "Looping through file  2015_05_01__2015_043073.txt\n",
      "Looping through file  2011_11_01__2012_003082.txt\n",
      "Looping through file  2015_12_16__2015_084042.txt\n",
      "Looping through file  2017_12_01__2017_084934.txt\n",
      "Looping through file  2013_06_01__2013_073146.txt\n",
      "Looping through file  2017_03_01__2017_039221.txt\n",
      "Looping through file  2017_03_01__2017_039341.txt\n",
      "Looping through file  2015_06_16__2015_069545.txt\n",
      "Looping through file  2013_11_15__2013_069396.txt\n",
      "Looping through file  2009_01_01__2009_016579.txt\n",
      "Looping through file  2014_01_01__2014_038109.txt\n",
      "Looping through file  2013_06_01__2013_073403.txt\n",
      "Looping through file  2010_04_01__2010_074416.txt\n",
      "Looping through file  2015_01_01__2015_065035.txt\n",
      "Looping through file  2017_03_01__2017_039364.txt\n",
      "Looping through file  2012_05_01__2012_032527.txt\n",
      "Looping through file  2009_01_01__2009_034089.txt\n",
      "Looping through file  2013_11_15__2013_068336.txt\n",
      "Looping through file  2018_05_01__2018_044198.txt\n",
      "Looping through file  2014_01_01__2014_081419.txt\n",
      "Looping through file  2013_06_01__2013_073410.txt\n",
      "Looping through file  2009_01_01__2009_016389.txt\n",
      "Looping through file  2010_04_01__2010_074884.txt\n",
      "Looping through file  2014_01_01__2014_038173.txt\n",
      "Looping through file  2012_05_01__2012_042462.txt\n",
      "Looping through file  2012_05_01__2012_042489.txt\n",
      "Looping through file  2016_09_01__2016_082062.txt\n",
      "Looping through file  2010_04_01__2010_075192.txt\n",
      "Looping through file  2015_05_01__2015_043120.txt\n",
      "Looping through file  2014_01_01__2014_081490.txt\n",
      "Looping through file  2015_06_16__2015_075846.txt\n",
      "Looping through file  2012_05_01__2012_032551.txt\n",
      "Looping through file  2010_04_01__2010_074659.txt\n",
      "Looping through file  2015_05_01__2015_043084.txt\n",
      "Looping through file  2015_12_16__2015_083965.txt\n",
      "Looping through file  2015_06_17__2015_031484.txt\n"
     ]
    }
   ],
   "source": [
    "# rewrites output file\n",
    "path_txt = os.path.join(cba_path, \"data.csv\")\n",
    "with io.open(path_txt,'w',encoding='utf8') as f:\n",
    "    header = 'contract_id|validity|theme|title|text'\n",
    "    f.write(header + '\\n')\n",
    "\n",
    "# loops over each contract\n",
    "for idx, files in enumerate(os.listdir(file_path)):\n",
    "    print(\"Looping through file \", files)\n",
    "    output_all(file_path, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_id</th>\n",
       "      <th>validity</th>\n",
       "      <th>theme</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>carimbo</th>\n",
       "      <th>semvalorlegal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Minimum Wage</td>\n",
       "      <td>- SALÁRIOS NORMATIVOS  Fica assegurado, para o...</td>\n",
       "      <td>salários normativos ficar assegurar empregado ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Adjustments / Corrections</td>\n",
       "      <td>Os salários dos empregados (as) serão corrigi...</td>\n",
       "      <td>salário empregado ser corrigir percentual vírg...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Adjustments / Corrections</td>\n",
       "      <td>COMPENSAÇÕES  Serão antes COMPENSADOS DA APLIC...</td>\n",
       "      <td>compensações serão compensados aplicação reaju...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Adjustments / Corrections</td>\n",
       "      <td>- ADMISSÃO APÓS DATA-BASE  O reajuste salarial...</td>\n",
       "      <td>admissão reajuste salarial empregado admitir o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016_082054</td>\n",
       "      <td>carimbo</td>\n",
       "      <td>Wages</td>\n",
       "      <td>Salary Payment</td>\n",
       "      <td>- PAGAMENTO DE SALÁRIOS  A) A empresa deverá p...</td>\n",
       "      <td>pagamento salários empresa proporcionar empreg...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contract_id validity  theme                             title  \\\n",
       "0  2016_082054  carimbo  Wages                      Minimum Wage   \n",
       "1  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
       "2  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
       "3  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
       "4  2016_082054  carimbo  Wages                    Salary Payment   \n",
       "\n",
       "                                                text  \\\n",
       "0  - SALÁRIOS NORMATIVOS  Fica assegurado, para o...   \n",
       "1   Os salários dos empregados (as) serão corrigi...   \n",
       "2  COMPENSAÇÕES  Serão antes COMPENSADOS DA APLIC...   \n",
       "3  - ADMISSÃO APÓS DATA-BASE  O reajuste salarial...   \n",
       "4  - PAGAMENTO DE SALÁRIOS  A) A empresa deverá p...   \n",
       "\n",
       "                                          clean_text  carimbo  semvalorlegal  \n",
       "0  salários normativos ficar assegurar empregado ...        1              0  \n",
       "1  salário empregado ser corrigir percentual vírg...        1              0  \n",
       "2  compensações serão compensados aplicação reaju...        1              0  \n",
       "3  admissão reajuste salarial empregado admitir o...        1              0  \n",
       "4  pagamento salários empresa proporcionar empreg...        1              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization package\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# adds custom stop words\n",
    "custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']\n",
    "for word in custom_stop_words:\n",
    "    stop_words.add(word)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# reads data into dataframe\n",
    "df = pd.read_csv(f'clause_analysis/data.csv', sep='|')\n",
    "\n",
    "# cleans the text\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# creates dummies for validity\n",
    "validity_dummies = pd.get_dummies(df['validity'])\n",
    "df = pd.concat([df, validity_dummies], axis=1)\n",
    "\n",
    "# saves csv file for analysis\n",
    "df.to_csv('clause_analysis/data.csv', sep='|')\n",
    "\n",
    "# displays head of csv\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  contract_id validity  theme                             title  \\\n",
      "0           0  2016_082054  carimbo  Wages                      Minimum Wage   \n",
      "1           1  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
      "2           2  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
      "3           3  2016_082054  carimbo  Wages  Salary Adjustments / Corrections   \n",
      "4           4  2016_082054  carimbo  Wages                    Salary Payment   \n",
      "\n",
      "                                                text  \\\n",
      "0  - SALÁRIOS NORMATIVOS  Fica assegurado, para o...   \n",
      "1   Os salários dos empregados (as) serão corrigi...   \n",
      "2  COMPENSAÇÕES  Serão antes COMPENSADOS DA APLIC...   \n",
      "3  - ADMISSÃO APÓS DATA-BASE  O reajuste salarial...   \n",
      "4  - PAGAMENTO DE SALÁRIOS  A) A empresa deverá p...   \n",
      "\n",
      "                                          clean_text  carimbo  semvalorlegal  \n",
      "0  salários normativos ficar assegurar empregado ...        1              0  \n",
      "1  salário empregado ser corrigir percentual vírg...        1              0  \n",
      "2  compensações serão compensados aplicação reaju...        1              0  \n",
      "3  admissão reajuste salarial empregado admitir o...        1              0  \n",
      "4  pagamento salários empresa proporcionar empreg...        1              0  \n",
      "Theme: Wages\n",
      "Best value of C: 1\n",
      "Test accuracy: 0.5428571428571428\n",
      "Top indicative features: ['útil', 'entretanto', 'dizer', 'diária', 'diário', 'diários', 'documento', 'doença', 'dolo', 'domicílio', 'domingo', 'domingos', 'dominical', 'doravante', 'dsr', 'durante', 'duração', 'duzento', 'débito', 'décima']\n",
      "     Unnamed: 0  contract_id       validity   theme  \\\n",
      "53           53  2016_082054        carimbo  Health   \n",
      "54           54  2016_082054        carimbo  Health   \n",
      "79           79  2012_002993  semvalorlegal  Health   \n",
      "101         101  2012_002993  semvalorlegal  Health   \n",
      "180         180  2012_002943  semvalorlegal  Health   \n",
      "\n",
      "                                  title  \\\n",
      "53                        Medical Exams   \n",
      "54   Acceptance of Medical Certificates   \n",
      "79                          Health Care   \n",
      "101                       Medical Exams   \n",
      "180                           First Aid   \n",
      "\n",
      "                                                  text  \\\n",
      "53   EXAME PREVENTIVO DE CÂNCER  A empresa que empr...   \n",
      "54   ATESTADOS MÉDICOS E ODONTOLÓGICOS  Serão recon...   \n",
      "79    Os Empregadores estão obrigados a instituir o...   \n",
      "101  Para efeito de justificação, abono de faltas e...   \n",
      "180  A empresa se obriga a comunicar aos familiares...   \n",
      "\n",
      "                                            clean_text  carimbo  semvalorlegal  \n",
      "53   exame preventivo câncer empresa empregar mão o...        1              0  \n",
      "54   atestados médicos odontológicos serão reconhec...        1              0  \n",
      "79   empregadores obrigado instituir convênio médic...        0              1  \n",
      "101  efeito justificação abono falta atraso empresa...        0              1  \n",
      "180  empresa obrigar comunicar familiar acidentado ...        0              1  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/lfh0pr6s18l0t4hm_q6nnb_m0000gn/T/ipykernel_4668/1519450563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mcv_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mcheck_cv\u001b[0;34m(cv, y, classifier)\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2302\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2303\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2304\u001b[0m         ):\n\u001b[1;32m   2305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for theme in themes: \n",
    "    # reads data into dataframe\n",
    "    df = pd.read_csv(f'clause_analysis/data.csv', sep='|')\n",
    "\n",
    "    # filters for theme\n",
    "    df = df[df['theme'] == theme]\n",
    "    print(df.head())\n",
    "\n",
    "    # calculatues TFIDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df['clean_text'])\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "    # splits the data for training and testing\n",
    "    y = df[df['theme'] == theme]['carimbo']\n",
    "    X = X.toarray()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=253)\n",
    "\n",
    "    # trains a logistic regression model with L1 penalty and liblinear solver\n",
    "    logreg = LogisticRegression(penalty='l1', solver='liblinear', random_state=253)\n",
    "\n",
    "    # finds the best value of C \n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    grid = GridSearchCV(logreg, param_grid, cv=2)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # prints the best value of C and the test accuracy\n",
    "    print(f\"Theme: {theme}\")\n",
    "    print(f\"Best value of C: {best_model.C}\")\n",
    "    print(f\"Test accuracy: {best_model.score(X_test, y_test)}\")\n",
    "\n",
    "    # gets the top indicative features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coefs = best_model.coef_[0]\n",
    "    top_indicative_features = [feature_names[i] for i in coefs.argsort()[-20:][::-1]]\n",
    "    print(\"Top indicative features:\", top_indicative_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
