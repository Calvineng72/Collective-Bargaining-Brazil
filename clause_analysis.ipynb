{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from itertools import dropwhile, takewhile\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the output directory\n",
    "cba_path = os.path.join(\".\", \"clause_data\")\n",
    "if not os.path.isdir(cba_path):\n",
    "    os.mkdir(cba_path)\n",
    "\n",
    "# sets the input directory\n",
    "file_path = os.getcwd() + '/cbas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_groups = pd.read_csv('clause_groups.csv', index_col='Clause Group')\n",
    "translation_dict = clause_groups['Translation'].to_dict()\n",
    "themes = list(map(str, clause_groups['Theme'].unique()))\n",
    "theme_dict = clause_groups['Theme'].to_dict()\n",
    "\n",
    "def extract_clauses(file_path, clause_type):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        # removes white space from the ends of lines\n",
    "        lines = (line.strip() for line in f)  \n",
    "    \n",
    "        # extracts the types of clauses present\n",
    "        clause_flag_start = dropwhile(lambda line: '<STARTofCLAUSES>' not in line, lines)\n",
    "        next(clause_flag_start,\"\")\n",
    "        clause_flag_end = takewhile(lambda line: '<ENDofCLAUSES>' not in line, clause_flag_start)\n",
    "        themes = []\n",
    "        titles = []\n",
    "        for line in clause_flag_end:\n",
    "            if not line: \n",
    "                continue  \n",
    "            title = line.split('|')[0]\n",
    "            translation = translation_dict[title]\n",
    "            titles.append(translation)\n",
    "            theme = theme_dict[title]\n",
    "            themes.append(theme)\n",
    "\n",
    "        # extracts the text of clauses\n",
    "        text_flag_start = dropwhile(lambda line: '<STARTofTEXT>' not in line, lines)\n",
    "        next(text_flag_start, \"\")\n",
    "        texts = []\n",
    "        text = []\n",
    "        for line in text_flag_start:\n",
    "            if '|' in line: \n",
    "                text.append(line.split('|')[0])\n",
    "                texts.append((' ').join(text))\n",
    "                text = []\n",
    "            else:\n",
    "                text.append(line)\n",
    "        if text:\n",
    "            texts.append((' ').join(text))\n",
    "\n",
    "        # retains clauses of proper type\n",
    "        indices_of_type = [i for i, theme in enumerate(themes) if theme == clause_type]\n",
    "        titles_of_type = [titles[i] for i in indices_of_type]\n",
    "        texts_of_type = [texts[i] for i in indices_of_type]\n",
    "\n",
    "        return titles_of_type, texts_of_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Non-compliance with Agreement', 'Renewal / Termination of the Agreement'],\n",
       " ['Em caso de descumprimento do presente acordo, a empresa pagará multa de um piso da categoria, que será revertido em favor do empregado prejudicado. ',\n",
       "  'O presente acordo deverá ter uma via depositada no órgão regional do\\xa0 Ministério do Trabalho, tendo validade pelo prazo de dois anos, a contar de 01/01/2014, podendo ser revogado ou prorrogado por outro acordo, conforme a conveniência das partes acordantes. E por estarem justas e acordadas as partes\\xa0 firmam o presente acordo em 03 (três) vias de igual forma e teor para que produza os efeitos legais. '])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_clauses('cbas/2014_01_01__2014_081501.txt', 'Contract Agreement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_all(file_path_x, files_x, clause_type):\n",
    "    # only considers files with start dates 2008-2017\n",
    "    if files_x[0:4].isdigit() and 2008 <= int(files_x[0:4]) <= 2017:\n",
    "        # contract identifier\n",
    "        contract_id = [files_x[-15:-4]]\n",
    "        if len(files_x[-15:-4]) != 11:\n",
    "            pass\n",
    "        titles, texts = extract_clauses(os.path.join(file_path_x, files_x), clause_type)\n",
    "        # saves info for contract as a single new line\n",
    "        pairs = [(contract_id + [title, text]) for title, text in zip(titles, texts)]\n",
    "        with io.open(path_txt, 'a', encoding='utf8') as f:\n",
    "            for pair in pairs:\n",
    "                pair_line = '|'.join(str(x) for x in pair)\n",
    "                f.write(pair_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme in themes: \n",
    "    file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "\n",
    "    # rewrites output file\n",
    "    path_txt = os.path.join(cba_path, f\"{file_name}_text.csv\")\n",
    "    with io.open(path_txt,'w',encoding='utf8') as f:\n",
    "        header = 'contract_id|title|text'\n",
    "        f.write(header + '\\n')\n",
    "\n",
    "    # loops over each contract\n",
    "    [output_all(file_path, files, theme) for files in os.listdir(file_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# stop_words = set(stopwords.words('portuguese'))\n",
    "# stemmer = SnowballStemmer('portuguese')\n",
    "# translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# def clean_text(text):\n",
    "#     tokens = word_tokenize(text, language='portuguese')\n",
    "#     tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "#     tokens = [stemmer.stem(word) for word in tokens]\n",
    "#     tokens = [word.translate(translator) for word in tokens]\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#     cleaned_text = ' '.join(tokens).lower()\n",
    "#     return cleaned_text\n",
    "\n",
    "# for theme in themes:\n",
    "#     file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "#     df = pd.read_csv(f'clause_data/{file_name}_text.csv', sep='|')\n",
    "#     df['clean_text'] = df['text'].apply(clean_text)\n",
    "#     df.to_csv(f'clause_data/{file_name}_text.csv', sep='|', index=False)\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "for theme in themes:\n",
    "    file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "    df = pd.read_csv(f'clause_data/{file_name}_text.csv', sep='|')\n",
    "    df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "    df.to_csv(f'clause_data/{file_name}_text.csv', sep='|', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme in themes:\n",
    "    file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "    df = pd.read_csv(f'clause_data/{file_name}_text.csv', sep='|')\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    df = pd.concat([df, tfidf_df], axis=1)\n",
    "    \n",
    "    df.to_csv(f'clause_data/{file_name}_tfidf.csv', sep='|', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: Wages\n",
      "salário      0.068194\n",
      "hora         0.060706\n",
      "ser          0.057867\n",
      "empregado    0.057133\n",
      "dia          0.050825\n",
      "pagamento    0.046515\n",
      "empresa      0.045579\n",
      "salarial     0.044186\n",
      "parágrafo    0.039754\n",
      "trabalho     0.038318\n",
      "dtype: float64\n",
      "Theme: Health\n",
      "médico          0.098278\n",
      "empregado       0.082917\n",
      "empresa         0.077869\n",
      "ser             0.066750\n",
      "exame           0.058755\n",
      "odontológico    0.058425\n",
      "atestado        0.057703\n",
      "convênio        0.056864\n",
      "profissional    0.050989\n",
      "parágrafo       0.045900\n",
      "dtype: float64\n",
      "Theme: Union\n",
      "empresa         0.067722\n",
      "sindicato       0.061966\n",
      "empregado       0.052740\n",
      "ser             0.051446\n",
      "dia             0.049210\n",
      "trabalho        0.045859\n",
      "profissional    0.040082\n",
      "sindical        0.038536\n",
      "acordo          0.036012\n",
      "contribuição    0.035511\n",
      "dtype: float64\n",
      "Theme: Safety / Injury / Disability\n",
      "empresa        0.076615\n",
      "empregado      0.070159\n",
      "dia            0.053347\n",
      "trabalho       0.051221\n",
      "ser            0.046898\n",
      "cipa           0.039258\n",
      "acidente       0.036000\n",
      "equipamento    0.033119\n",
      "proteção       0.031817\n",
      "risco          0.029636\n",
      "dtype: float64\n",
      "Theme: Work Adaptation / Training\n",
      "serviço        0.098747\n",
      "empregado      0.097291\n",
      "militar        0.084906\n",
      "empresa        0.080250\n",
      "prestação      0.069604\n",
      "dia            0.068995\n",
      "idade          0.068882\n",
      "emprego        0.065311\n",
      "alistamento    0.062435\n",
      "trabalho       0.061551\n",
      "dtype: float64\n",
      "Theme: Work Time\n",
      "hora         0.093867\n",
      "dia          0.084118\n",
      "trabalho     0.059540\n",
      "ser          0.055730\n",
      "empregado    0.054113\n",
      "empresa      0.045984\n",
      "parágrafo    0.041198\n",
      "jornada      0.036186\n",
      "férias       0.035171\n",
      "remunerar    0.031568\n",
      "dtype: float64\n",
      "Theme: Incentives\n",
      "empresa      0.095147\n",
      "empregado    0.068801\n",
      "ser          0.068486\n",
      "serviço      0.062845\n",
      "pagar        0.053027\n",
      "dia          0.049128\n",
      "salário      0.046417\n",
      "mesmo        0.042646\n",
      "real         0.042469\n",
      "trabalho     0.041333\n",
      "dtype: float64\n",
      "Theme: Food / Education / Housing\n",
      "empregado     0.079866\n",
      "empresa       0.076045\n",
      "transporte    0.067238\n",
      "fornecer      0.058677\n",
      "parágrafo     0.057232\n",
      "ser           0.052271\n",
      "nº            0.051929\n",
      "real          0.044296\n",
      "trabalho      0.043235\n",
      "empregador    0.043038\n",
      "dtype: float64\n",
      "Theme: Contract Agreement\n",
      "acordo            0.089461\n",
      "multa             0.066084\n",
      "presente          0.065851\n",
      "cláusula          0.064088\n",
      "trabalho          0.057822\n",
      "coletivo          0.054147\n",
      "descumprimento    0.049506\n",
      "empregado         0.047619\n",
      "empresa           0.047104\n",
      "sindicato         0.044822\n",
      "dtype: float64\n",
      "Theme: Retirement\n",
      "empregado        0.109613\n",
      "aposentadoria    0.106210\n",
      "empresa          0.104115\n",
      "ano              0.095791\n",
      "serviço          0.088477\n",
      "período          0.080903\n",
      "mínimo           0.073925\n",
      "prazo            0.073399\n",
      "direito          0.071237\n",
      "salário          0.068214\n",
      "dtype: float64\n",
      "Theme: Work Environment / Harassment\n",
      "uniforme         0.114490\n",
      "empregado        0.105892\n",
      "empresa          0.096184\n",
      "uso              0.095345\n",
      "exigir           0.092804\n",
      "gratuitamente    0.089065\n",
      "fornecer         0.058960\n",
      "trabalho         0.056860\n",
      "água             0.055498\n",
      "ficar            0.051395\n",
      "dtype: float64\n",
      "Theme: Family\n",
      "empresa      0.081562\n",
      "empregado    0.072882\n",
      "empregada    0.056788\n",
      "dia          0.054600\n",
      "salário      0.048693\n",
      "seguro       0.048586\n",
      "caso         0.047117\n",
      "parágrafo    0.044162\n",
      "ser          0.042528\n",
      "trabalho     0.040400\n",
      "dtype: float64\n",
      "Theme: Dismissals / Transfers\n",
      "empregado    0.079426\n",
      "aviso        0.075035\n",
      "empresa      0.071139\n",
      "dia          0.069415\n",
      "prévio       0.068155\n",
      "ano          0.068107\n",
      "ser          0.063122\n",
      "trabalho     0.060787\n",
      "rescisão     0.059200\n",
      "dispensa     0.046878\n",
      "dtype: float64\n",
      "Theme: Fees\n",
      "trabalhar         0.268995\n",
      "dia               0.219677\n",
      "pagamento         0.219677\n",
      "ser               0.169117\n",
      "citar             0.134497\n",
      "comissionista     0.134497\n",
      "comissionistas    0.134497\n",
      "expediente        0.134497\n",
      "ficar             0.134497\n",
      "garantido         0.134497\n",
      "dtype: float64\n",
      "Theme: Staffing / Hiring / Outsourcing\n",
      "empresa        0.082504\n",
      "empregado      0.071761\n",
      "função         0.058956\n",
      "contrato       0.057121\n",
      "ser            0.055821\n",
      "salário        0.052999\n",
      "mesmo          0.048547\n",
      "dia            0.046593\n",
      "trabalho       0.046480\n",
      "experiência    0.045462\n",
      "dtype: float64\n",
      "Theme: Other\n",
      "empregado    0.059595\n",
      "empresa      0.050566\n",
      "trabalho     0.045390\n",
      "dia          0.042914\n",
      "ser          0.038319\n",
      "salário      0.033984\n",
      "hora         0.031840\n",
      "parágrafo    0.030413\n",
      "ficar        0.027580\n",
      "acordo       0.026241\n",
      "dtype: float64\n",
      "Theme: Equality / Fairness\n",
      "ser             0.075173\n",
      "pagar           0.068059\n",
      "participação    0.067336\n",
      "empregado       0.066689\n",
      "empresa         0.065685\n",
      "dia             0.063158\n",
      "lucros          0.056743\n",
      "pagamento       0.052582\n",
      "programa        0.052224\n",
      "lucro           0.048488\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "    df = pd.read_csv(f'clause_data/{file_name}_tfidf.csv', sep='|')\n",
    "    \n",
    "    tfidf_cols = [col for col in df.columns if col not in ['text', 'clean_text']]\n",
    "    tfidf_means = df.select_dtypes(include=['float64']).mean()\n",
    "    top_ten = tfidf_means.nlargest(10)\n",
    "    \n",
    "    print(f'Theme: {theme}')\n",
    "    print(top_ten)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
