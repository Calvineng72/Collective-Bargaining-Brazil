{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from itertools import dropwhile, takewhile\n",
    "\n",
    "# more stop words, valid/not valid, acordo coletivo and not ammendments (extrato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the output directory\n",
    "cba_path = os.path.join(\".\", \"clause_data\")\n",
    "if not os.path.isdir(cba_path):\n",
    "    os.mkdir(cba_path)\n",
    "\n",
    "# sets the input directory\n",
    "# file_path = os.getcwd() + '/cbas'\n",
    "file_path = '/Users/calvineng/Dropbox/Calvin_Eng/cba_text_analysis/cba_txt_2009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theme and translation dictionaries for clause_groups\n",
    "clause_groups = pd.read_csv('clause_groups.csv', index_col='Clause Group')\n",
    "translation_dict = clause_groups['Translation'].to_dict()\n",
    "themes = list(map(str, clause_groups['Theme'].unique()))\n",
    "theme_dict = clause_groups['Theme'].to_dict()\n",
    "\n",
    "# retrieves the type of document\n",
    "def extract_document_type(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        lines = (line.strip() for line in f)   \n",
    "        title_start_flage = dropwhile(lambda line: '<STARTofTITLE>' not in line, lines)\n",
    "        next(title_start_flage,\"\")\n",
    "        title_end_flag = takewhile(lambda line: '<ENDofTITLE>' not in line, title_start_flage)\n",
    "        title = ''.join(title_end_flag).strip()\n",
    "        if 'Extrato Acordo Coletivo' in title:\n",
    "            acordo, extrato = 1, 1\n",
    "        elif 'Extrato Convenção Coletiva' in title:\n",
    "            acordo, extrato = 0, 1\n",
    "        elif 'Extrato Termo Aditivo de Acordo Coletivo' in title:\n",
    "            acordo, extrato = 1, 0\n",
    "        elif 'Extrato Termo Aditivo de Convenção Coletiva' in title:\n",
    "            acordo, extrato = 0, 0\n",
    "        else:\n",
    "            acordo, extrato = '', ''\n",
    "\n",
    "    return acordo, extrato\n",
    "\n",
    "# retrieves the validity\n",
    "def extract_validity(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        lines = (line.strip() for line in f) \n",
    "        validity_start_flag = dropwhile(lambda line: '<STARTofVALIDITY>' not in line, lines)\n",
    "        next(validity_start_flag,\"\")\n",
    "        validity_end_flag = takewhile(lambda line: '<ENDofVALIDITY>' not in line, validity_start_flag)\n",
    "        validity = ''.join(validity_end_flag).strip()\n",
    "        if 'carimbo' in validity:\n",
    "            validity = 1\n",
    "        elif 'semvalorlegal' in validity:\n",
    "            validity = 0\n",
    "        else:\n",
    "            validity = ''\n",
    "\n",
    "    return validity\n",
    "\n",
    "# extracts the types of clauses present\n",
    "def extract_clause_names(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        names = []\n",
    "        lines = (line.strip() for line in f)      \n",
    "        clause_flag_start = dropwhile(lambda line: '<STARTofCLAUSES>' not in line, lines)\n",
    "        next(clause_flag_start,\"\")\n",
    "        clause_flag_end = takewhile(lambda line: '<ENDofCLAUSES>' not in line, clause_flag_start)\n",
    "        for line in clause_flag_end:\n",
    "            if not line: \n",
    "                continue\n",
    "            title = line.split('|')[0]\n",
    "            if title not in theme_dict:\n",
    "                continue\n",
    "            translation = translation_dict[title]\n",
    "            names.append(translation)\n",
    "\n",
    "    return names\n",
    "\n",
    "# extracts the text of clauses\n",
    "def extract_clause_texts(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        text = []\n",
    "        texts = []\n",
    "        lines = (line.strip() for line in f)  \n",
    "        text_flag_start = dropwhile(lambda line: '<STARTofTEXT>' not in line, lines)\n",
    "        next(text_flag_start, \"\")\n",
    "        for line in text_flag_start:\n",
    "            if '|' in line: \n",
    "                text.append(line.split('|')[0])\n",
    "                texts.append(('').join(text).replace('\\xa0','').strip().lower())\n",
    "                text = [line.split('|')[1]]\n",
    "            else:\n",
    "                text.append(line)\n",
    "        if text:\n",
    "            texts.append(('').join(text).replace('\\xa0','').strip().lower())\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_all(file_path_x, files_x):\n",
    "    # only considers files with start dates 2008-2017\n",
    "    if files_x[0:4].isdigit() and 2008 <= int(files_x[0:4]) <= 2017:\n",
    "        # contract identifier\n",
    "        contract_id = [files_x[-15:-4]]\n",
    "        if len(files_x[-15:-4]) != 11:\n",
    "            pass\n",
    "\n",
    "        # extracts information from document\n",
    "        file_path = os.path.join(file_path_x, files_x)\n",
    "        acordo, extrato = extract_document_type(file_path)\n",
    "        validity = extract_validity(file_path)\n",
    "        names = extract_clause_names(file_path)\n",
    "        texts = extract_clause_texts(file_path)\n",
    "\n",
    "        # saves info for contract as a single new line\n",
    "        pairs = [(contract_id + [acordo, extrato, validity, name, text]) for name, text in zip(names, texts)]\n",
    "        with io.open(path_txt, 'a', encoding='utf8') as f:\n",
    "            for pair in pairs:\n",
    "                pair_line = '|'.join(str(x) for x in pair)\n",
    "                f.write(pair_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through file  2009_11_01__2010_033261.txt\n",
      "Looping through file  2009_06_01__2010_009417.txt\n",
      "Looping through file  2009_10_01__2009_057913.txt\n",
      "Looping through file  2009_09_01__2010_055133.txt\n",
      "Looping through file  2009_11_01__2009_061078.txt\n",
      "Looping through file  2009_03_20__2009_040361.txt\n",
      "Looping through file  2009_04_01__2009_030683.txt\n",
      "Looping through file  2009_05_01__2009_040365.txt\n",
      "Looping through file  2009_04_01__2009_014862.txt\n",
      "Looping through file  2009_04_01__2009_010972.txt\n",
      "Looping through file  2009_01_01__2009_052824.txt\n",
      "Looping through file  2009_05_01__2009_023162.txt\n",
      "Looping through file  2009_08_01__2009_064476.txt\n",
      "Looping through file  2009_12_18__2010_051882.txt\n",
      "Looping through file  2009_02_06__2009_003866.txt\n",
      "Looping through file  2009_05_01__2009_018184.txt\n",
      "Looping through file  2009_09_22__2009_046008.txt\n",
      "Looping through file  2009_06_01__2009_031139.txt\n",
      "Looping through file  2009_04_01__2009_043786.txt\n",
      "Looping through file  2009_03_24__2009_031070.txt\n",
      "Looping through file  2009_03_01__2009_010905.txt\n",
      "Looping through file  2009_09_16__2009_045614.txt\n",
      "Looping through file  2009_04_01__2009_060376.txt\n",
      "Looping through file  2009_01_01__2009_050894.txt\n",
      "Looping through file  2009_03_01__2009_009263.txt\n",
      "Looping through file  2009_05_01__2009_026290.txt\n",
      "Looping through file  2009_05_01__2009_032032.txt\n",
      "Looping through file  2009_05_01__2009_017113.txt\n",
      "Looping through file  2009_05_01__2009_025349.txt\n",
      "Looping through file  2009_09_01__2010_016050.txt\n",
      "Looping through file  2009_03_23__2009_009879.txt\n",
      "Looping through file  2009_03_12__2009_008194.txt\n",
      "Looping through file  2009_05_01__2009_029543.txt\n",
      "Looping through file  2009_01_01__2009_040121.txt\n",
      "Looping through file  2009_09_01__2010_049572.txt\n",
      "Looping through file  2009_12_03__2009_060548.txt\n",
      "Looping through file  2009_11_01__2010_030310.txt\n",
      "Looping through file  2009_06_01__2009_025768.txt\n",
      "Looping through file  2009_08_01__2011_016793.txt\n",
      "Looping through file  2009_01_01__2009_008771.txt\n",
      "Looping through file  2009_05_01__2009_051987.txt\n",
      "Looping through file  2009_06_16__2009_012040.txt\n",
      "Looping through file  2009_06_01__2009_029551.txt\n",
      "Looping through file  2009_12_04__2010_001554.txt\n",
      "Looping through file  2009_05_01__2009_026503.txt\n",
      "Looping through file  2009_09_01__2009_047507.txt\n",
      "Looping through file  2009_07_15__2009_056860.txt\n",
      "Looping through file  2009_09_01__2009_042052.txt\n",
      "Looping through file  2009_01_01__2009_005422.txt\n",
      "Looping through file  2009_09_01__2009_042656.txt\n",
      "Looping through file  2009_09_01__2009_049449.txt\n",
      "Looping through file  2009_11_01__2010_010114.txt\n",
      "Looping through file  2009_09_01__2010_008038.txt\n",
      "Looping through file  2009_10_01__2009_053986.txt\n",
      "Looping through file  2009_12_21__2009_064540.txt\n",
      "Looping through file  2009_05_01__2009_040816.txt\n",
      "Looping through file  2009_09_01__2010_011026.txt\n",
      "Looping through file  2009_04_27__2009_016533.txt\n",
      "Looping through file  2009_11_01__2010_000261.txt\n",
      "Looping through file  2009_02_01__2009_039109.txt\n"
     ]
    }
   ],
   "source": [
    "# rewrites output file\n",
    "path_txt = os.path.join(cba_path, \"clause_text.csv\")\n",
    "with io.open(path_txt,'w',encoding='utf8') as f:\n",
    "    header = 'contract_id|acordo|extrato|validity|title|text'\n",
    "    f.write(header + '\\n')\n",
    "\n",
    "# loops over each contract\n",
    "for idx, files in enumerate(os.listdir(file_path)):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Looping through file \", files)\n",
    "    output_all(file_path, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/calvineng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/calvineng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "Top 20 words for \"\":\n",
      "['empreg', 'trabalh', 'empres', 'hor', 'salári', 'acord', 'dias', 'dev', 'fic', 'cent', 'part', 'dia', 'pagament', 'sindicat', 'valor', 'cas', 'colet', 'present', 'servic', 'pod']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "stemmer = SnowballStemmer('portuguese')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# adds custom stop words\n",
    "custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']\n",
    "stop_words.extend(custom_stop_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    tokens = [word.translate(translator) for word in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    cleaned_text = ' '.join(tokens).lower()\n",
    "    return cleaned_text\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# # lemmatization package\n",
    "# nlp = spacy.load('pt_core_news_sm')\n",
    "# stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# # adds custom stop words\n",
    "# custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']\n",
    "# stop_words.extend(custom_stop_words)\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     try: \n",
    "#         doc = nlp(text)\n",
    "#     except:\n",
    "#         return ''\n",
    "#     tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# reads file as csv\n",
    "df = pd.read_csv(f'clause_data/clause_text.csv', sep='|', nrows=10000)\n",
    "\n",
    "# cleans the text\n",
    "for i in range(len(df)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    df.at[i, 'clean_text'] = clean_text(df.at[i, 'text'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# calculatues TFIDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# selects 20 tokens with highest TFIDF\n",
    "tfidf_cols = [col for col in df.columns if col not in ['text', 'clean_text']]\n",
    "tfidf_means = df.select_dtypes(include=['float64']).mean()\n",
    "top_twenty = tfidf_means.nlargest(20)\n",
    "\n",
    "# prints the top 20 words\n",
    "print(f'Top 20 words for \"\":')\n",
    "print(list(top_twenty.index))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for \"Wages\":\n",
      "['salário', 'hora', 'ser', 'empregado', 'dia', 'pagamento', 'empresa', 'salarial', 'trabalho', 'real', 'pagar', 'cláusula', 'adiantamento', 'ficar', 'compensação', 'acordo', 'reajuste', 'piso', 'desconto', 'efetuar']\n",
      "\n",
      "Top 20 words for \"Health\":\n",
      "['médico', 'empregado', 'empresa', 'ser', 'exame', 'odontológico', 'atestado', 'convênio', 'profissional', 'caso', 'serviço', 'trabalho', 'empregador', 'sindicato', 'saúde', 'dia', 'plano', 'reconhecer', 'estar', 'fornecer']\n",
      "\n",
      "Top 20 words for \"Union\":\n",
      "['empresa', 'sindicato', 'empregado', 'ser', 'dia', 'trabalho', 'profissional', 'sindical', 'acordo', 'contribuição', 'desconto', 'categoria', 'entidade', 'descontar', 'trabalhador', 'coletivo', 'salário', 'empregador', 'pagamento', 'prazo']\n",
      "\n",
      "Top 20 words for \"Safety / Injury / Disability\":\n",
      "['empresa', 'empregado', 'dia', 'trabalho', 'ser', 'cipa', 'acidente', 'equipamento', 'proteção', 'risco', 'uso', 'prazo', 'fornecer', 'dever', 'sindicato', 'individual', 'ficar', 'caso', 'profissional', 'período']\n",
      "\n",
      "Top 20 words for \"Work Adaptation / Training\":\n",
      "['serviço', 'empregado', 'militar', 'empresa', 'prestação', 'dia', 'idade', 'emprego', 'alistamento', 'trabalho', 'salário', 'guerra', 'tiro', 'ser', 'trinto', 'estabilidade', 'ficar', 'função', 'data', 'garantir']\n",
      "\n",
      "Top 20 words for \"Work Time\":\n",
      "['hora', 'dia', 'trabalho', 'ser', 'empregado', 'empresa', 'jornada', 'férias', 'remunerar', 'ficar', 'trabalhar', 'período', 'caso', 'adicional', 'lei', 'normal', 'acordo', 'licença', 'útil', 'domingo']\n",
      "\n",
      "Top 20 words for \"Incentives\":\n",
      "['empresa', 'empregado', 'ser', 'serviço', 'pagar', 'dia', 'salário', 'mesmo', 'real', 'trabalho', 'ano', 'alimentação', 'ficar', 'receber', 'trabalhador', 'premio', 'domingo', 'refeição', 'caixa', 'ter']\n",
      "\n",
      "Top 20 words for \"Food / Education / Housing\":\n",
      "['empregado', 'empresa', 'transporte', 'fornecer', 'ser', 'real', 'trabalho', 'empregador', 'trabalhador', 'lei', 'dia', 'alimentação', 'vale', 'serviço', 'ficar', 'salário', 'decreto', 'refeição', 'conceder', 'despesa']\n",
      "\n",
      "Top 20 words for \"Contract Agreement\":\n",
      "['acordo', 'multa', 'presente', 'cláusula', 'trabalho', 'coletivo', 'descumprimento', 'empregado', 'empresa', 'sindicato', 'prejudicar', 'ser', 'categoria', 'caso', 'ficar', 'parte', 'condição', 'reverter', 'empregar', 'convenção']\n",
      "\n",
      "Top 20 words for \"Retirement\":\n",
      "['empregado', 'aposentadoria', 'empresa', 'ano', 'serviço', 'período', 'mínimo', 'prazo', 'direito', 'salário', 'trabalhador', 'emprego', 'ficar', 'aquisição', 'aposentar', 'estar', 'dia', 'idade', 'caso', 'garantia']\n",
      "\n",
      "Top 20 words for \"Work Environment / Harassment\":\n",
      "['uniforme', 'empregado', 'empresa', 'uso', 'exigir', 'gratuitamente', 'fornecer', 'trabalho', 'água', 'ficar', 'acordo', 'empregador', 'fornecimento', 'condição', 'ser', 'manter', 'potável', 'fornecir', 'gratuito', 'mesmo']\n",
      "\n",
      "Top 20 words for \"Family\":\n",
      "['empresa', 'empregado', 'empregada', 'dia', 'seguro', 'salário', 'caso', 'ser', 'trabalho', 'cláusula', 'estabilidade', 'gestante', 'licença', 'auxílio', 'falecimento', 'ficar', 'vida', 'conforme', 'idade', 'justa']\n",
      "\n",
      "Top 20 words for \"Dismissals / Transfers\":\n",
      "['empregado', 'aviso', 'empresa', 'dia', 'prévio', 'ano', 'ser', 'trabalho', 'rescisão', 'dispensa', 'contrato', 'justa', 'causa', 'serviço', 'escrito', 'demissão', 'caso', 'homologação', 'prazo', 'dispensar']\n",
      "\n",
      "Top 20 words for \"Fees\":\n",
      "['trabalhar', 'dia', 'pagamento', 'ser', 'citar', 'comissionista', 'comissionistas', 'expediente', 'ficar', 'garantido', 'garantir', 'pagar', 'remunerado', 'trabalhadores', 'vendedor', 'acima', 'real', 'repouso', 'semanal', 'empresa']\n",
      "\n",
      "Top 20 words for \"Staffing / Hiring / Outsourcing\":\n",
      "['empresa', 'empregado', 'função', 'contrato', 'ser', 'salário', 'mesmo', 'dia', 'trabalho', 'experiência', 'caso', 'prazo', 'ctps', 'contratação', 'hora', 'trabalhador', 'teste', 'rescisão', 'ficar', 'pagamento']\n",
      "\n",
      "Top 20 words for \"Other\":\n",
      "['empregado', 'empresa', 'trabalho', 'dia', 'ser', 'salário', 'hora', 'ficar', 'acordo', 'sindicato', 'pagamento', 'presente', 'empregador', 'caso', 'período', 'profissional', 'ano', 'trabalhador', 'ter', 'contrato']\n",
      "\n",
      "Top 20 words for \"Equality / Fairness\":\n",
      "['ser', 'pagar', 'participação', 'empregado', 'empresa', 'dia', 'lucros', 'pagamento', 'programa', 'lucro', 'plr', 'período', 'substituição', 'função', 'presente', 'trabalho', 'resultados', 'instrumento', 'estabelecer', 'resultado']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    # reads file as csv\n",
    "    file_name = theme.lower().replace(' / ', '_').replace(' ', '_')\n",
    "    df = pd.read_csv(f'clause_data/{file_name}_text.csv', sep='|')\n",
    "\n",
    "    # selects 20 tokens with highest TFIDF\n",
    "    tfidf_cols = [col for col in df.columns if col not in ['text', 'clean_text']]\n",
    "    tfidf_means = df.select_dtypes(include=['float64']).mean()\n",
    "    top_twenty = tfidf_means.nlargest(20)\n",
    "\n",
    "    # prints the top 20 words\n",
    "    print(f'Top 20 words for \"{theme}\":')\n",
    "    print(list(top_twenty.index))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
