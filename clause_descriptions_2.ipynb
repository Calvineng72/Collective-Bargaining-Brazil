{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TXT files to CSV files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the directories for the input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# sets the output directory\n",
    "cba_path = os.path.join(\".\", \"clause_data\")\n",
    "if not os.path.isdir(cba_path):\n",
    "    os.mkdir(cba_path)\n",
    "\n",
    "# sets the input directory\n",
    "# file_path = os.getcwd() + '/cbas'\n",
    "file_path = '/Users/calvineng/Dropbox/Calvin_Eng/cba_text_analysis/cba_txt_2009'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to store the translations and subgroups of clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['13th month bonus', 'Acceptance of medical certificates', 'Access to company information', 'Union access to workplace', 'Accompaniment of work-related injured worker', 'Adaptation of work functions', 'Night pay', 'Overtime pay', 'Hazard pay (health risk)', 'Shift pay', 'Hazard pay (danger risk)', 'On-call pay', 'Seniority pay', 'Subsistence allowance', 'Application of the CBA', 'Retirement', 'Moral harassment', 'Sexual harassment', 'Assignment to (deviation from) work functions', 'Work authorization on Sundays and holidays', 'Food assistance', 'Childcare assistance', 'Illness/disability assistance', 'Education assistance', 'Housing assistance', 'Maternity assistance', 'Death/funeral assistance', 'Health assistance', 'Transportation assistance', 'Performance evaluation', 'Advance notice', 'CIPA: accident prevention committee', 'Health education campaigns', 'Factory commission', 'Fees', 'Workday compensation', 'Working environment conditions', 'Part-time contracts', 'Union fees', 'Workday controls', 'Weekly rest', 'Salary deductions', 'Non-compliance with the CBA', 'Separation/dismissal', 'Right of opposition to union fees', 'Vacation duration and concession', 'Duration and schedule', 'Loans', 'Equipments for individual safety', 'Safety equipment', 'Abortion protections', 'Work-related injury protections', 'Adoption protections', 'Retirement protections', 'Apprenticeship protections', 'Employment protections', 'Maternity protections', 'Paternity protections', 'Nonwork-related injury protections', 'Military service protections', 'Internship/apprenticeship', 'Medical exams', 'Absences', 'Tools and equipment', 'Collective vacations', 'Guarantees to union officers', 'Guarantees for nonwork-related injured workers', 'Work function bonus', 'Equal opportunities', 'Insalubrity', 'Break intervals', 'Wage isonomy', 'Special days (women, minors, students)', 'Union activities leave', 'Abortion leave', 'Adoption leave', 'Maternity leave', 'Paid leave', 'Unpaid leave', 'Machine and equipment maintenance', 'Mechanisms for conflict resolution', 'Female workforce', 'Youth workforce', 'Outsourcing/temporary workforce', 'Advanced-age workforce', 'Disciplinary norms', 'Employment/hiring rules', 'Other provisions', 'Other bonuses', 'Other standards for the prevention of injuries', 'Other standards for protection of injured workers', 'Other provisions on holidays and leaves', 'Other provisions on the workday', 'Other provisions on the relationship between union and company', 'Other provisions on representation and organization', 'Other employment protections', 'Other staffing rules', 'Other rules regarding admission, dismissal, and contracting', 'Other rules regarding conditions for carrying out work functions', 'Other rules regarding salaries, adjustments, and payments', 'Other pays', 'Other assistances', 'Other specific worker groups', 'Salary payment - means and timeframes', 'Employee participation in business management', 'Profit sharing', 'Hazard (danger risk)', 'Minimum wage', 'Schedule of tasks and wages', 'Policy for dependents', 'Policies for employment maintenance', 'People with special needs', 'First aid', 'Procedures in relation to strikes and strikers', 'Health and safety professionals', 'PPE: employment protection program', 'Extension/reduction of workday', 'Awards', 'Vocational training/qualification', 'Rehabilitation of the injured', 'Salary adjustments/corrections', 'Rules for negotiating', 'DSR: weekly rest remuneration', 'Holiday remuneration', 'Renewal/termination of the CBA', 'Union representative', 'Internship/apprenticeship salary', 'Family salary', 'Production or task salary', 'Life insurance', 'Unionization (campaigns and hiring of union members)', 'On-call', 'Suspension of employment contract', 'Transfers', 'Training for work-related injury prevention', 'Uninterrupted shifts', 'Uniforms', 'Blank', 'Non-identified']\n",
      "['Bonuses', 'Protections', 'Union-firm relations', 'Staffing rules', 'Pay', 'Assistances', 'General provisions', 'Other income', 'Working conditions', 'Workday', 'Separations', 'Prevention', 'Union organization', 'Contract types', 'Wage adjustment', 'Holidays', 'Employment protections', 'Leave', 'Hiring', 'Other: holidays and leave', 'Other: employment contract', 'Other: adjustments, payments, wages', 'Wage payment', 'Other wages', 'Not-identified']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# theme and translation dictionaries for clause_groups\n",
    "clause_groups = pd.read_csv('clause_groups/clause_groups_NEW.csv', index_col='cl_subgrp_pt')\n",
    "translation_dict = clause_groups['cl_subgrp_en'].to_dict()\n",
    "translations = list(map(str, clause_groups['cl_subgrp_en'].unique()))\n",
    "subgroup_dict = clause_groups['cl_grp_en'].to_dict()\n",
    "subgroups = list(map(str, clause_groups['cl_grp_en'].unique()))\n",
    "print(translations)\n",
    "print(subgroups)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that extract various details from collective bargaining agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from itertools import dropwhile, takewhile\n",
    "\n",
    "# retrieves the type of document\n",
    "def extract_document_type(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        lines = (line.strip() for line in f)   \n",
    "        title_start_flage = dropwhile(lambda line: '<STARTofTITLE>' not in line, lines)\n",
    "        next(title_start_flage,\"\")\n",
    "        title_end_flag = takewhile(lambda line: '<ENDofTITLE>' not in line, title_start_flage)\n",
    "        title = ''.join(title_end_flag).strip()\n",
    "        if 'Extrato Acordo Coletivo' in title:\n",
    "            acordo, extrato = 1, 1\n",
    "        elif 'Extrato Convenção Coletiva' in title:\n",
    "            acordo, extrato = 0, 1\n",
    "        elif 'Extrato Termo Aditivo de Acordo Coletivo' in title:\n",
    "            acordo, extrato = 1, 0\n",
    "        elif 'Extrato Termo Aditivo de Convenção Coletiva' in title:\n",
    "            acordo, extrato = 0, 0\n",
    "        else:\n",
    "            acordo, extrato = '', ''\n",
    "\n",
    "    return acordo, extrato\n",
    "\n",
    "# retrieves the validity\n",
    "def extract_validity(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        lines = (line.strip() for line in f) \n",
    "        validity_start_flag = dropwhile(lambda line: '<STARTofVALIDITY>' not in line, lines)\n",
    "        next(validity_start_flag,\"\")\n",
    "        validity_end_flag = takewhile(lambda line: '<ENDofVALIDITY>' not in line, validity_start_flag)\n",
    "        validity = ''.join(validity_end_flag).strip()\n",
    "        if 'carimbo' in validity:\n",
    "            validity = 1\n",
    "        elif 'semvalorlegal' in validity:\n",
    "            validity = 0\n",
    "        else:\n",
    "            validity = ''\n",
    "\n",
    "    return validity\n",
    "\n",
    "# extracts the types of clauses present\n",
    "def extract_clause_names(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        names = []\n",
    "        subgroups = []\n",
    "        lines = (line.strip() for line in f)      \n",
    "        clause_flag_start = dropwhile(lambda line: '<STARTofCLAUSES>' not in line, lines)\n",
    "        next(clause_flag_start,\"\")\n",
    "        clause_flag_end = takewhile(lambda line: '<ENDofCLAUSES>' not in line, clause_flag_start)\n",
    "        for line in clause_flag_end:\n",
    "            if not line: \n",
    "                continue\n",
    "            try: \n",
    "                title = line.split('|')[0]\n",
    "                translation = translation_dict[title]\n",
    "                subgroup = subgroup_dict[title]\n",
    "            except:\n",
    "                translation = ''\n",
    "                subgroup =''\n",
    "            names.append(translation)\n",
    "            subgroups.append(subgroup)\n",
    "\n",
    "    return names, subgroups\n",
    "\n",
    "# extracts the text of clauses\n",
    "def extract_clause_texts(file_path):\n",
    "    with io.open(file_path, 'r') as f:\n",
    "        text = []\n",
    "        texts = []\n",
    "        lines = (line.strip() for line in f)  \n",
    "        text_flag_start = dropwhile(lambda line: '<STARTofTEXT>' not in line, lines)\n",
    "        next(text_flag_start, \"\")\n",
    "        for line in text_flag_start:\n",
    "            if '|' in line: \n",
    "                text.append(line.split('|')[0])\n",
    "                texts.append(('').join(text).replace('\\xa0','').strip())\n",
    "                text = [line.split('|')[1]]\n",
    "            else:\n",
    "                text.append(line)\n",
    "        if text:\n",
    "            texts.append(('').join(text).replace('\\xa0','').strip())\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that outputs information from collective bargaining agreements in the form of a CSV with File I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_all(file_path_x, files_x):\n",
    "    # only considers files with start dates 2008-2017\n",
    "    if files_x[0:4].isdigit() and 2008 <= int(files_x[0:4]) <= 2017:\n",
    "        # contract identifier\n",
    "        contract_id = [files_x[-15:-4]]\n",
    "        if len(files_x[-15:-4]) != 11:\n",
    "            pass\n",
    "\n",
    "        # extracts information from document\n",
    "        file_path = os.path.join(file_path_x, files_x)\n",
    "        acordo, extrato = extract_document_type(file_path)\n",
    "        validity = extract_validity(file_path)\n",
    "        names, subgroups = extract_clause_names(file_path)\n",
    "        texts = extract_clause_texts(file_path)\n",
    "\n",
    "        # saves info for contract as a single new line\n",
    "        pairs = [(contract_id + [acordo, extrato, validity, name, subgroup, text]) for name, subgroup, text in zip(names, subgroups, texts)]\n",
    "        with io.open(path_txt, 'a', encoding='utf8') as f:\n",
    "            for pair in pairs:\n",
    "                pair_line = '|'.join(str(x) for x in pair)\n",
    "                f.write(pair_line + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through collective bargaining agreements to create the output for the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through file  2009_11_01__2010_033261.txt\n",
      "Looping through file  2009_06_01__2010_009417.txt\n",
      "Looping through file  2009_10_01__2009_057913.txt\n",
      "Looping through file  2009_09_01__2010_055133.txt\n",
      "Looping through file  2009_11_01__2009_061078.txt\n",
      "Looping through file  2009_03_20__2009_040361.txt\n",
      "Looping through file  2009_04_01__2009_030683.txt\n",
      "Looping through file  2009_05_01__2009_040365.txt\n",
      "Looping through file  2009_04_01__2009_014862.txt\n",
      "Looping through file  2009_04_01__2009_010972.txt\n",
      "Looping through file  2009_01_01__2009_052824.txt\n",
      "Looping through file  2009_05_01__2009_023162.txt\n",
      "Looping through file  2009_08_01__2009_064476.txt\n",
      "Looping through file  2009_12_18__2010_051882.txt\n",
      "Looping through file  2009_02_06__2009_003866.txt\n",
      "Looping through file  2009_05_01__2009_018184.txt\n",
      "Looping through file  2009_09_22__2009_046008.txt\n",
      "Looping through file  2009_06_01__2009_031139.txt\n",
      "Looping through file  2009_04_01__2009_043786.txt\n",
      "Looping through file  2009_03_24__2009_031070.txt\n",
      "Looping through file  2009_03_01__2009_010905.txt\n",
      "Looping through file  2009_09_16__2009_045614.txt\n",
      "Looping through file  2009_04_01__2009_060376.txt\n",
      "Looping through file  2009_01_01__2009_050894.txt\n",
      "Looping through file  2009_03_01__2009_009263.txt\n",
      "Looping through file  2009_05_01__2009_026290.txt\n",
      "Looping through file  2009_05_01__2009_032032.txt\n",
      "Looping through file  2009_05_01__2009_017113.txt\n",
      "Looping through file  2009_05_01__2009_025349.txt\n",
      "Looping through file  2009_09_01__2010_016050.txt\n",
      "Looping through file  2009_03_23__2009_009879.txt\n",
      "Looping through file  2009_03_12__2009_008194.txt\n",
      "Looping through file  2009_05_01__2009_029543.txt\n",
      "Looping through file  2009_01_01__2009_040121.txt\n",
      "Looping through file  2009_09_01__2010_049572.txt\n",
      "Looping through file  2009_12_03__2009_060548.txt\n",
      "Looping through file  2009_11_01__2010_030310.txt\n",
      "Looping through file  2009_06_01__2009_025768.txt\n",
      "Looping through file  2009_08_01__2011_016793.txt\n",
      "Looping through file  2009_01_01__2009_008771.txt\n",
      "Looping through file  2009_05_01__2009_051987.txt\n",
      "Looping through file  2009_06_16__2009_012040.txt\n",
      "Looping through file  2009_06_01__2009_029551.txt\n",
      "Looping through file  2009_12_04__2010_001554.txt\n",
      "Looping through file  2009_05_01__2009_026503.txt\n",
      "Looping through file  2009_09_01__2009_047507.txt\n",
      "Looping through file  2009_07_15__2009_056860.txt\n",
      "Looping through file  2009_09_01__2009_042052.txt\n",
      "Looping through file  2009_01_01__2009_005422.txt\n",
      "Looping through file  2009_09_01__2009_042656.txt\n",
      "Looping through file  2009_09_01__2009_049449.txt\n",
      "Looping through file  2009_11_01__2010_010114.txt\n",
      "Looping through file  2009_09_01__2010_008038.txt\n",
      "Looping through file  2009_10_01__2009_053986.txt\n",
      "Looping through file  2009_12_21__2009_064540.txt\n",
      "Looping through file  2009_05_01__2009_040816.txt\n",
      "Looping through file  2009_09_01__2010_011026.txt\n",
      "Looping through file  2009_04_27__2009_016533.txt\n",
      "Looping through file  2009_11_01__2010_000261.txt\n",
      "Looping through file  2009_02_01__2009_039109.txt\n"
     ]
    }
   ],
   "source": [
    "# rewrites output file\n",
    "path_txt = os.path.join(cba_path, \"contract_clauses.csv\")\n",
    "with io.open(path_txt,'w',encoding='utf8') as f:\n",
    "    header = 'contract_id|acordo|extrato|validity|name|subgroup|text'\n",
    "    f.write(header + '\\n')\n",
    "\n",
    "# loops over each contract\n",
    "for idx, files in enumerate(os.listdir(file_path)):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Looping through file \", files)\n",
    "    output_all(file_path, files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data for valid documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_id</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>Minimum wage</td>\n",
       "      <td>Ficará garantido ao empregado motorista o valo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>Salary payment - means and timeframes</td>\n",
       "      <td>Para as funções de motorista de carreta, bi-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>Salary deductions</td>\n",
       "      <td>Qualquer multa por excesso de velocidade, por ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>Food assistance</td>\n",
       "      <td>Os empregados motoristas externos receberão me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009_055971</td>\n",
       "      <td>Food assistance</td>\n",
       "      <td>Levando-se em conta a crise econômica e a redu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contract_id                                   name  \\\n",
       "0  2009_055971                           Minimum wage   \n",
       "1  2009_055971  Salary payment - means and timeframes   \n",
       "2  2009_055971                      Salary deductions   \n",
       "3  2009_055971                        Food assistance   \n",
       "4  2009_055971                        Food assistance   \n",
       "\n",
       "                                                text  \n",
       "0  Ficará garantido ao empregado motorista o valo...  \n",
       "1  Para as funções de motorista de carreta, bi-tr...  \n",
       "2  Qualquer multa por excesso de velocidade, por ...  \n",
       "3  Os empregados motoristas externos receberão me...  \n",
       "4  Levando-se em conta a crise econômica e a redu...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reads file as csv\n",
    "df = pd.read_csv('clause_data/contract_clauses.csv', sep=\"|\")\n",
    "\n",
    "# keeps only valid ACTs\n",
    "df = df.loc[(df['acordo'] == 1)&(df['extrato'] == 1)&(df['validity'] == 1)]\n",
    "df = df.drop(['acordo', 'extrato', 'validity', 'subgroup'], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "# reindexes the dataframe with the default integer index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(len(df.index))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SnowballStemmer for Portuguese and ntlk package for stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/calvineng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/calvineng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# adds custom stop words\n",
    "custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x',\n",
    "                     'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx', 'xxi',\n",
    "                     'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxix', 'xxx', 'number',\n",
    "                     'clt', 'artigo']\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "# stemmer\n",
    "stemmer = SnowballStemmer('portuguese')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer to preprocess documents with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CBApreprocess(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, preprocess=True, remove_punctuation=True,\n",
    "                 replace_numbers=True, remove_stopwords=True, stemming=True):\n",
    "        self.preprocess = preprocess\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stemming = stemming\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]|º', '', text)\n",
    "        if self.replace_numbers:\n",
    "            text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "        if self.remove_stopwords:\n",
    "            words = text.split()\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "            text = ' '.join(words)\n",
    "        if self.stemming and stemmer is not None:\n",
    "            words = text.split()\n",
    "            words = [stemmer.stem(word) for word in words]\n",
    "            text = ' '.join(words)\n",
    "        return text.lower()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = [\n",
    "            self.preprocess_text(cba)\n",
    "            for cba in X\n",
    "        ]\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the preprocessing with stemming on the first five documents in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fic garant empreg motor valor pis salarial categor cas venh trabalh integral mês eou fiqu disposiçã consig ating pis salarial valor comissõ cas trabalh parcial pagament pis proporcional'\n",
      " 'funçõ motor carret bitr tritr rod trem treminhã simil fic acert remuner comissõ pur valor equivalent comissõ propri dit eos reflex repous seman remuner feri som total cinc virgul setent cinc cent fatur liqu caminhã transport realiz dentr estad min geraisparágraf primeir part consid fatur líqu valor fret brut deduçõ valor impost agenc carg pedági send veícul carregadoparágraf segund fic acert cinc virgul setent cinc cent percentual comissõ acert sext referes pagament reflex comissõ sobr repous seman remuner eventu feri send recib pagamento valor refer percentu desmembr títul comissõ comissõ sobr rsrs sobr feri aparec respect discrimin separadasparágraf terceir fic acert transport realiz estad min ger som percetnu comissõ reflex comissõ sobr repous seman remuner feri result seis virgul setent cinc cent send cert dest percentual sext referes mesm reflex acim cit'\n",
      " 'qualqu mult excess veloc ausênc disc tacógraf excess pes culp obreir respond mesm desd contrat autoriz contrat proced respect débit pagament mens acert final trct independent proporçã relaçã crédit'\n",
      " 'empreg motor extern receb mensal trabalh desenvolv dentr estad min ger espéc equivalent dois virgul setent cinc cent fatur líqu títul ajud cust art cobertur valor títul aliment ajud aliment estad diár viagens refer pod const recib pagament títul ajud alimentaçãoajud cust eou diár viag transport realiz estad min ger percentual ajud cust pass três virgul vint cinc cent valor parcel mencion itens anterior integr remuner motor efeit incident fgts salári fér adicion parcel indenizatór époc demissã contribuiçõ previdenciár'\n",
      " 'levandos cont cris econôm reduçã servic ped part client empres acord compromet present cláusul fornec cad motor cest básic mensal iniciandos primeir entreg mês agost finaliz mês dezembr cont seguint itens pcts arroz kg tip prat fin kg caf fin grã kg farinh mandioc torr kg fub momis sinh pct farinh trig ros branc especial lat ervilh numberg lat milh verd numberg pct goiab numberg goiás verd lat extrat tomat elef numberg kg feijã carioc tip kg sal refin pcts açuc cristal kg lat óle soj numberml pct biscoit cre crack numberg lat sardinh cmolh numberg kg macarrã espaguet pct macarrã ave mar sêmol chiarin numberg pcts biscoit mar aymor numberg pct biscoit amanteig numberg cx sabã pó omo numberkg frs detergent ypê numberml sabonet lux lux numberg pct papel higiên personal cnumb unid pct bombril crem dental numberg colgatesorrisoparágraf primeir confecçã cest tend produt marc descrit substituíd marc qualidad equivalenteparágraf segund part decl benefíci carát provisóri incorpor remuner empreg nenhum efeit']\n",
      "['Ficará garantido ao empregado motorista o valor do piso salarial da categoria, caso venha trabalhar integralmente no mês e/ou fique à disposição e não consiga atingir o Piso Salarial com os valores de suas comissões. No caso de trabalho parcial, o pagamento do piso será proporcional.'\n",
      " 'Para as funções de motorista de carreta, bi-trem, tri-trem, rodo trem ou treminhão ou similares, fica acertado que a remuneração será por comissões puras no valor equivalente entre as comissões propriamente dita eos reflexos nos repousos semanais remunerados e feriados somarão o total de 5,75% (cinco virgula setenta e cinco por cento) do faturamento liquido do caminhão, para o transporte realizado para dentro do Estado de Minas Gerais.PARÁGRAFO PRIMEIRO: As partes consideram como faturamento líquido os valores dos fretes brutos, com as deduções dos valores dos impostos, agenciamentos de cargas e pedágio, sendo este, quando o veículo estiver carregado.PARÁGRAFO SEGUNDO: Fica acertado que dos 5,75% (cinco virgula setenta e cinco por cento) do percentual das comissões acertadas, 1/6 (um sexto) refere-se ao pagamento dos reflexos das comissões sobre os repousos semanais remunerados e eventuais feriados, sendo que nos recibos de pagamentoos valores dos referidos percentuais serão desmembrados com os títulos de ´´comissões´´ e ´´comissões sobre R.S.Rs e sobre feriados´´ e aparecerão com as respectivas discriminações em separadas.PARÁGRAFO TERCEIRO: Fica acertado que quando o transporte for realizado fora do Estado de Minas Gerais, a soma dos percetnuais de comissões e reflexos das comissões sobre os repousos semanais remunerados e feriados resultarão 6,75% (seis virgula setenta e cinco por cento), sendo certo que, deste percentual 1/6 (um sexto) refere-se aos mesmos reflexos acima citados.'\n",
      " 'Qualquer multa por excesso de velocidade, por ausência de discos de tacógrafo, por excesso de peso por culpa do obreiro, responderá este pelas mesmas, e, desde já, o contratado autoriza à contratante a proceder aos respectivos débitos nos pagamentos mensais ou no acerto final (TRCT), independente da proporção em relação aos seus créditos.'\n",
      " 'Os empregados motoristas externos receberão mensalmente, pelo trabalho desenvolvido dentro do Estado de Minas Gerais, em espécie, o equivalente e 2,75% (dois virgula setenta e cinco por cento) de seu faturamento líquido, a título de ajuda de custo (art. 457, parágrafo 2º da CLT), para cobertura de valores a títulos de : alimentação, ajuda alimentação, estadia e diárias para viagens, e os referidos poderão constar nos recibos de pagamento com os títulos ajuda alimentação/ajuda de custo e/ou diária de viagem.- Para os transportes realizados fora do Estado de Minas Gerais, o percentual de ajuda de custo passará a ser de 3,25% (três virgula vinte e cinco por cento).- Os valores e parcelas mencionados nos itens anteriores não integrarão a remuneração do motorista para os efeitos de incidência de FGTS, 13º salários, férias mais adicionais, em parcelas indenizatórias por época da demissão e contribuições previdenciárias'\n",
      " 'Levando-se em conta a crise econômica e a redução dos serviços e de pedidos por parte de clientes, a empresa acordante se compromete , pela presente cláusula, a fornecer, a cada motorista, uma cesta básica mensal, iniciando-se a primeira entrega no mês de agosto de 2009, e finalizando no mês de dezembro de 2009, contendo os seguintes itens:- 02 pcts arroz 05 KG tipo 1 - Prato Fino;- 01 KG de café fino grão;- 01 KG farinha mandioca torrada;- 01 KG fubá momiso Sinhá;- 01 pct farinha trigo Rosa Branca especial;- 01 lata ervilha 200G;- 02 latas milho verde 200G;- 01 pct goiabada 500G Goiás Verde;- 01 lata extrato tomate elefante 340G;- 03 KG feijão carioca tipo 1;- 01 KG sal refinado;- 02 pcts Açucar cristal 5 KG;- 02 latas óleo soja 900ml;- 01 pct biscoito cream cracker 200G;- 01 lata sardinha c/molho 132G;- 01 KG macarrão espaguete;- 01 pct macarrão Ave Maria Sêmola Chiarini 500G;- 02 pcts Biscoito Maria Aymoré 200G;- 01 pct Biscoito amanteigado 400G;- 01 cx. Sabão em pó Omo 1KG;- 04 frs. Detergente Ypê 500ml;- 04 sabonetes Lux Luxo 90G;- 01 pct Papel Higiênico Personal c/08 unid;- 01 pct Bombril;- 01 creme dental 180G Colgate/Sorriso.PARÁGRAFO PRIMEIRO: Na confecção das cestas, não tendo os produtos das marcas descritas, serão substituídos por marcas de qualidade equivalente.PARÁGRAFO SEGUNDO: As partes declaram que o benefício tem o caráter provisório e não se incorporará na remuneração do empregado para nenhum efeito.']\n"
     ]
    }
   ],
   "source": [
    "X_few = df['text'].iloc[:5]\n",
    "cba_preprocessor = CBApreprocess()\n",
    "X_few_processed = cba_preprocessor.fit_transform(X_few)\n",
    "print(X_few_processed)\n",
    "print(X_few.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer to calculate the TFIDF for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class CBAToTFIDFTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_tfidf = self.vectorizer.transform(X)\n",
    "        return X_tfidf.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the TFIDF on the first five documents in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18216425, 0.3643285 , 0.18216425, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.09801016, 0.04900508,\n",
       "        0.04900508]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_wordcounts = CBAToTFIDFTransformer().fit_transform(X_few_processed)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer for Portuguese from SpaCy and stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# lemmatizer\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# stop words\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# adds custom stop words\n",
    "custom_stop_words = ['parágrafo', 'nº', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x',\n",
    "                     'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx', 'xxi',\n",
    "                     'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxix', 'xxx', 'number',\n",
    "                     'clt', 'artigo']\n",
    "stop_words.update(custom_stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer to preprocess documents with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAPreprocessLemmatize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, preprocess=True, remove_punctuation=True,\n",
    "                 replace_numbers=True, remove_stopwords=True, lemmatize=True):\n",
    "        self.preprocess = preprocess\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]|º', '', text)\n",
    "        if self.replace_numbers:\n",
    "            text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "        if self.remove_stopwords:\n",
    "            words = text.split()\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "            text = ' '.join(words)\n",
    "        if self.lemmatize:\n",
    "            doc = nlp(text)\n",
    "            words = [token.lemma_ for token in doc]\n",
    "            text = ' '.join(words)\n",
    "        return text.lower()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = [\n",
    "            self.preprocess_text(cba)\n",
    "            for cba in X\n",
    "        ]\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the preprocessing with lemmatization on the first five documents in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ficar garantir empregar motorista piso salarial categor caso vir trabalhar integralmente ear fique disposição consiga atingir piso salarial valor comissão caso trabalho parcial pagamento piso ser proporcional'\n",
      " 'função motorista carreta bitr tritr rodo tr treminhão similar ficar acertar remuneração ser comissão pur equivalente comissão propriamente dizer eos reflexo repouso semanal remunerado feriado somar total virgula setenta faturamento liquido caminhão transporte realizar minas geraisparágrafo parte considerar faturamento líquido valor frete bruto dedução valor imposto agenciamento carga pedágio ser veículo estar carregadoparágrafo fica acertar virgula setenta percentual comissão acertar refereser pagamento reflexo comissão repouso semanal remunerado eventual feriado ser recibo pagamentoos valor referir percentual ser desmembrar título comissão comissão rsrs feriar aparecer respectivo discriminação separadasparágrafo fica acertar transporte realizar minas gerais somar percetnual comissão reflexo comissão repouso semanal remunerado feriado resultar virgula setenta ser certo percentual refereser mesmo reflexo acima citar'\n",
      " 'multa excesso velocidade ausênciar disco tacógrafo excesso pesar culpa obreiro responder mesma contratar autorizar contratante proceder respectivo débito pagamento mensal acerto trct independente proporção crédito'\n",
      " 'empregado motorista externo receber mensalmente trabalho desenvolver minas gerais espécie equivalente virgulir setenta faturamento líquido título ajudar custo art cobertura valor título alimentação ajudar alimentação estadir diária viagem referir poder constar recibo pagamento título ajudar alimentaçãoajudar custo ear diário viagem transporte realizar minas gerais percentual ajudar custo passar virgula valor parcela mencionar item anterior integrarão remuneração motorista efeito incidênciar fgts salário férias adicional parcela indenizatório época demissão contribuição previdenciário'\n",
      " 'levandose contar crise econômico redução serviço pedido cliente empresa acordante comprometar presente cláusula fornecer motorista cesta básico mensal iniciandose entrega agosto finalizar dezembro conter seguinte item pcts arroz kg prato fino kg café fino gr kg farinha mandioca torrar kg fubá momiso sinhá pct farinha trigo rosa branca especial lata ervilha numberg lata milho verde numberg pct goiabar numberg goiás verde lata extrato tomate elefante numberg kg feijão carioca kg sal refinar pcts açucar cristal kg lata óleo soja numberml pct biscoito cream cracker numberg lata sardinhar cmolho numberg kg macarr espaguete pct macarr ave maria sêmola chiarini numberg pcts biscoito maria aymoré numberg pct biscoito amanteigar numberg cx sabão pó omo numberkg frs detergente ypê numberml sabonetes lux luxo numberg pct papel higiênico personal cnumber unid pct bombril creme dental numberg colgatesorrisoparágrafo confecção cesto ter produto marca descrita ser substituír marca qualidade equivalenteparágrafo parte declarar benefício caráter provisório incorporar remuneração empregar nenhum efeito']\n",
      "['Ficará garantido ao empregado motorista o valor do piso salarial da categoria, caso venha trabalhar integralmente no mês e/ou fique à disposição e não consiga atingir o Piso Salarial com os valores de suas comissões. No caso de trabalho parcial, o pagamento do piso será proporcional.'\n",
      " 'Para as funções de motorista de carreta, bi-trem, tri-trem, rodo trem ou treminhão ou similares, fica acertado que a remuneração será por comissões puras no valor equivalente entre as comissões propriamente dita eos reflexos nos repousos semanais remunerados e feriados somarão o total de 5,75% (cinco virgula setenta e cinco por cento) do faturamento liquido do caminhão, para o transporte realizado para dentro do Estado de Minas Gerais.PARÁGRAFO PRIMEIRO: As partes consideram como faturamento líquido os valores dos fretes brutos, com as deduções dos valores dos impostos, agenciamentos de cargas e pedágio, sendo este, quando o veículo estiver carregado.PARÁGRAFO SEGUNDO: Fica acertado que dos 5,75% (cinco virgula setenta e cinco por cento) do percentual das comissões acertadas, 1/6 (um sexto) refere-se ao pagamento dos reflexos das comissões sobre os repousos semanais remunerados e eventuais feriados, sendo que nos recibos de pagamentoos valores dos referidos percentuais serão desmembrados com os títulos de ´´comissões´´ e ´´comissões sobre R.S.Rs e sobre feriados´´ e aparecerão com as respectivas discriminações em separadas.PARÁGRAFO TERCEIRO: Fica acertado que quando o transporte for realizado fora do Estado de Minas Gerais, a soma dos percetnuais de comissões e reflexos das comissões sobre os repousos semanais remunerados e feriados resultarão 6,75% (seis virgula setenta e cinco por cento), sendo certo que, deste percentual 1/6 (um sexto) refere-se aos mesmos reflexos acima citados.'\n",
      " 'Qualquer multa por excesso de velocidade, por ausência de discos de tacógrafo, por excesso de peso por culpa do obreiro, responderá este pelas mesmas, e, desde já, o contratado autoriza à contratante a proceder aos respectivos débitos nos pagamentos mensais ou no acerto final (TRCT), independente da proporção em relação aos seus créditos.'\n",
      " 'Os empregados motoristas externos receberão mensalmente, pelo trabalho desenvolvido dentro do Estado de Minas Gerais, em espécie, o equivalente e 2,75% (dois virgula setenta e cinco por cento) de seu faturamento líquido, a título de ajuda de custo (art. 457, parágrafo 2º da CLT), para cobertura de valores a títulos de : alimentação, ajuda alimentação, estadia e diárias para viagens, e os referidos poderão constar nos recibos de pagamento com os títulos ajuda alimentação/ajuda de custo e/ou diária de viagem.- Para os transportes realizados fora do Estado de Minas Gerais, o percentual de ajuda de custo passará a ser de 3,25% (três virgula vinte e cinco por cento).- Os valores e parcelas mencionados nos itens anteriores não integrarão a remuneração do motorista para os efeitos de incidência de FGTS, 13º salários, férias mais adicionais, em parcelas indenizatórias por época da demissão e contribuições previdenciárias'\n",
      " 'Levando-se em conta a crise econômica e a redução dos serviços e de pedidos por parte de clientes, a empresa acordante se compromete , pela presente cláusula, a fornecer, a cada motorista, uma cesta básica mensal, iniciando-se a primeira entrega no mês de agosto de 2009, e finalizando no mês de dezembro de 2009, contendo os seguintes itens:- 02 pcts arroz 05 KG tipo 1 - Prato Fino;- 01 KG de café fino grão;- 01 KG farinha mandioca torrada;- 01 KG fubá momiso Sinhá;- 01 pct farinha trigo Rosa Branca especial;- 01 lata ervilha 200G;- 02 latas milho verde 200G;- 01 pct goiabada 500G Goiás Verde;- 01 lata extrato tomate elefante 340G;- 03 KG feijão carioca tipo 1;- 01 KG sal refinado;- 02 pcts Açucar cristal 5 KG;- 02 latas óleo soja 900ml;- 01 pct biscoito cream cracker 200G;- 01 lata sardinha c/molho 132G;- 01 KG macarrão espaguete;- 01 pct macarrão Ave Maria Sêmola Chiarini 500G;- 02 pcts Biscoito Maria Aymoré 200G;- 01 pct Biscoito amanteigado 400G;- 01 cx. Sabão em pó Omo 1KG;- 04 frs. Detergente Ypê 500ml;- 04 sabonetes Lux Luxo 90G;- 01 pct Papel Higiênico Personal c/08 unid;- 01 pct Bombril;- 01 creme dental 180G Colgate/Sorriso.PARÁGRAFO PRIMEIRO: Na confecção das cestas, não tendo os produtos das marcas descritas, serão substituídos por marcas de qualidade equivalente.PARÁGRAFO SEGUNDO: As partes declaram que o benefício tem o caráter provisório e não se incorporará na remuneração do empregado para nenhum efeito.']\n"
     ]
    }
   ],
   "source": [
    "X_few = df['text'].iloc[:5]\n",
    "cba_preprocessor = CBAPreprocessLemmatize()\n",
    "X_few_processed = cba_preprocessor.fit_transform(X_few)\n",
    "print(X_few_processed)\n",
    "print(X_few.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 words for each clause type using stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/lfh0pr6s18l0t4hm_q6nnb_m0000gn/T/ipykernel_52720/2570795538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmean_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtop_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_top_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Top {n_top_words} words for '{name}': {top_words}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'dictionary'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"cba_preprocess\", CBApreprocess()),\n",
    "    (\"cba_to_tfidf\", CBAToTFIDFTransformer())\n",
    "])\n",
    "\n",
    "# number of words to output and groupings\n",
    "groups = df.groupby('name')\n",
    "n_top_words = 20\n",
    "\n",
    "for name, group in groups:\n",
    "    data_texts = group['text'].tolist()\n",
    "    data_tfidf = preprocess_pipeline.fit_transform(data_texts)\n",
    "    mean_tfidf = data_tfidf.mean(axis=0)\n",
    "    top_indices = np.argsort(mean_tfidf)[-n_top_words:]\n",
    "    vocab = preprocess_pipeline.named_steps['cba_to_tfidf'].vectorizer.get_feature_names_out()\n",
    "    top_words = [vocab[idx] for idx in top_indices]\n",
    "    print(f\"Top {n_top_words} words for '{name}': {top_words}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 words for each clause type using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CBAPreprocessLemmatize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/lfh0pr6s18l0t4hm_q6nnb_m0000gn/T/ipykernel_52720/4034239029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# creates Pipeline to preprocess and calculate TF-IDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m preprocess_pipeline = Pipeline([\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"cba_preprocess\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCBAPreprocessLemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"cba_to_tfidf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCBAToTFIDFTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CBAPreprocessLemmatize' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# creates Pipeline to preprocess and calculate TF-IDF\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"cba_preprocess\", CBAPreprocessLemmatize()),\n",
    "    (\"cba_to_tfidf\", CBAToTFIDFTransformer())\n",
    "])\n",
    "\n",
    "# number of words to output and groupings\n",
    "groups = df.groupby('name')\n",
    "n_top_words = 20\n",
    "\n",
    "for name, group in groups:\n",
    "    data_texts = group['text'].tolist()\n",
    "    data_tfidf = preprocess_pipeline.fit_transform(data_texts)\n",
    "    mean_tfidf = data_tfidf.mean(axis=0)\n",
    "    top_indices = np.argsort(mean_tfidf)[-n_top_words:]\n",
    "    vocab = preprocess_pipeline.named_steps['cba_to_tfidf'].vectorizer.get_feature_names_out()\n",
    "    top_words = [vocab[idx] for idx in top_indices]\n",
    "    print(f\"Top {n_top_words} words for '{name}': {top_words}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 words for each clause subgroup using stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for 'Assistances': ['acord', 'fic', 'pagament', 'dependent', 'benefíci', 'conced', 'rea', 'val', 'auxíli', 'aliment', 'pag', 'cas', 'transport', 'ser', 'fornec', 'salári', 'trabalh', 'empres', 'empreg', 'numb']\n",
      "Top 20 words for 'Bonuses': ['ano', 'receb', 'caix', 'promoçã', 'funçã', 'adiant', 'gratific', 'cinquent', 'pagament', 'parcel', 'dia', 'dias', 'pag', 'trabalh', 'ser', 'fér', 'empres', 'salári', 'empreg', 'numb']\n",
      "Top 20 words for 'Contract types': ['estagiári', 'acord', 'lei', 'aprendizag', 'períod', 'mesm', 'funçã', 'superior', 'estági', 'praz', 'ser', 'salári', 'dias', 'aprendiz', 'trabalh', 'empres', 'experient', 'empreg', 'contrat', 'numb']\n",
      "Top 20 words for 'Employment protections': ['just', 'provisór', 'afast', 'anos', 'gestant', 'salári', 'períod', 'aposentador', 'milit', 'praz', 'assegur', 'fic', 'empres', 'trabalh', 'estabil', 'garant', 'servic', 'dias', 'numb', 'empreg']\n",
      "Top 20 words for 'General provisions': ['dirim', 'descumpr', 'justic', 'salári', 'estabelec', 'cas', 'aplic', 'ser', 'sindicat', 'part', 'fic', 'mult', 'empres', 'cláusul', 'colet', 'present', 'empreg', 'acord', 'trabalh', 'numb']\n",
      "Top 20 words for 'Hiring': ['servic', 'admissã', 'fic', 'praz', 'exerc', 'dias', 'cas', 'anot', 'acord', 'admit', 'ser', 'mesm', 'salári', 'experient', 'funçã', 'trabalh', 'empres', 'contrat', 'numb', 'empreg']\n",
      "Top 20 words for 'Holidays': ['trint', 'antecedent', 'trabalh', 'comunic', 'compens', 'dia', 'individu', 'sáb', 'coincid', 'doming', 'goz', 'feri', 'períod', 'empres', 'colet', 'iníci', 'empreg', 'dias', 'fér', 'numb']\n",
      "Top 20 words for 'Leave': ['trabalh', 'ano', 'fic', 'empres', 'idad', 'adot', 'gestant', 'remuner', 'judicial', 'adoçã', 'cas', 'ser', 'guard', 'conced', 'anos', 'crianc', 'licenc', 'dias', 'empreg', 'numb']\n",
      "Top 20 words for 'Other income': ['rea', 'cas', 'present', 'lucr', 'vid', 'aposentador', 'anos', 'servic', 'pag', 'pagament', 'salári', 'ser', 'result', 'acord', 'particip', 'segur', 'trabalh', 'empres', 'empreg', 'numb']\n",
      "Top 20 words for 'Other wages': ['correspondent', 'comission', 'descans', 'dias', 'can', 'cort', 'lei', 'feri', 'hor', 'empres', 'repous', 'durant', 'aprendiz', 'empreg', 'semanal', 'ser', 'remuner', 'salári', 'trabalh', 'numb']\n",
      "Top 20 words for 'Other: adjustments, payments, wages': ['dias', 'salarial', 'substituiçã', 'comprov', 'adiant', 'remuner', 'efetu', 'dia', 'funçã', 'fornec', 'descont', 'hor', 'pag', 'ser', 'trabalh', 'pagament', 'empres', 'salári', 'empreg', 'numb']\n",
      "Top 20 words for 'Other: employment contract': ['rescisã', 'cas', 'ctps', 'fic', 'caus', 'just', 'mesm', 'funçã', 'anot', 'fornec', 'ser', 'acord', 'praz', 'dispens', 'salári', 'contrat', 'trabalh', 'empres', 'empreg', 'numb']\n",
      "Top 20 words for 'Other: holidays and leave': ['assegur', 'colet', 'iníci', 'incis', 'trabalh', 'fic', 'ser', 'remuner', 'empres', 'úte', 'compens', 'feri', 'dia', 'art', 'licenc', 'casament', 'fér', 'empreg', 'dias', 'numb']\n",
      "Top 20 words for 'Pay': ['cinquent', 'extraordinár', 'cem', 'trint', 'salári', 'empres', 'dia', 'pag', 'acréscim', 'extras', 'serã', 'normal', 'ser', 'empreg', 'remuner', 'noturn', 'adicional', 'trabalh', 'numb', 'hor']\n",
      "Top 20 words for 'Prevention': ['cip', 'serã', 'condiçõ', 'mant', 'fic', 'exam', 'médic', 'acident', 'segur', 'proteçã', 'equip', 'uso', 'exig', 'gratuit', 'uniform', 'fornec', 'numb', 'trabalh', 'empres', 'empreg']\n",
      "Top 20 words for 'Protections': ['afast', 'reconhec', 'fornec', 'cas', 'convêni', 'profissional', 'serã', 'servic', 'dias', 'emit', 'aceit', 'sindicat', 'acident', 'odontológ', 'trabalh', 'empreg', 'empres', 'numb', 'atest', 'médic']\n",
      "Top 20 words for 'Separations': ['servic', 'homolog', 'motiv', 'fic', 'ser', 'contrat', 'rescisã', 'cas', 'anos', 'escrit', 'just', 'caus', 'dias', 'prévi', 'dispens', 'empres', 'avis', 'trabalh', 'empreg', 'numb']\n",
      "Top 20 words for 'Staffing rules': ['contrat', 'profissional', 'fic', 'hor', 'acord', 'períod', 'dev', 'ser', 'funçã', 'cas', 'dias', 'carg', 'curs', 'solicit', 'fornec', 'salári', 'trabalh', 'numb', 'empres', 'empreg']\n",
      "Top 20 words for 'Union organization': ['folh', 'dias', 'ser', 'assoc', 'entidad', 'pagament', 'dia', 'sindical', 'salári', 'acord', 'mensal', 'profissional', 'trabalh', 'contribuiçã', 'recolh', 'empres', 'sindicat', 'empreg', 'descont', 'numb']\n",
      "Top 20 words for 'Union-firm relations': ['represent', 'colet', 'diretor', 'dirigent', 'afix', 'sindic', 'categor', 'sindical', 'comunic', 'entidad', 'dias', 'quadr', 'profissional', 'acord', 'avis', 'trabalh', 'empreg', 'sindicat', 'numb', 'empres']\n",
      "Top 20 words for 'Wage adjustment': ['períod', 'aument', 'mai', 'aplic', 'colet', 'descont', 'conced', 'serã', 'fic', 'ser', 'pis', 'acord', 'empres', 'rea', 'trabalh', 'salarial', 'reajust', 'empreg', 'salári', 'numb']\n",
      "Top 20 words for 'Wage payment': ['subsequent', 'remuner', 'cont', 'chequ', 'discrimin', 'comprov', 'fornec', 'ser', 'trabalh', 'descont', 'adiant', 'pag', 'útil', 'efetu', 'empres', 'empreg', 'dia', 'salári', 'pagament', 'numb']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# creates Pipeline to preprocess and calculate TF-IDF\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"cba_preprocess\", CBApreprocess()),\n",
    "    (\"cba_to_tfidf\", CBAToTFIDFTransformer())\n",
    "])\n",
    "\n",
    "# number of words to output and groupings\n",
    "groups = df.groupby('subgroup')\n",
    "n_top_words = 20\n",
    "\n",
    "for name, group in groups:\n",
    "    data_texts = group['text'].tolist()\n",
    "    data_tfidf = preprocess_pipeline.fit_transform(data_texts)\n",
    "    mean_tfidf = data_tfidf.mean(axis=0)\n",
    "    top_indices = np.argsort(mean_tfidf)[-n_top_words:]\n",
    "    vocab = preprocess_pipeline.named_steps['cba_to_tfidf'].vectorizer.get_feature_names_out()\n",
    "    top_words = [vocab[idx] for idx in top_indices]\n",
    "    print(f\"Top {n_top_words} words for '{name}': {top_words}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 words for each clause subgroup using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for 'Assistances': ['filho', 'conceder', 'dependente', 'pagamento', 'benefício', 'real', 'alimentação', 'fornecer', 'auxílio', 'pagar', 'trabalho', 'transporte', 'caso', 'empregado', 'dia', 'salário', 'ser', 'empregar', 'empresa', 'number']\n",
      "Top 20 words for 'Bonuses': ['ocasião', 'ficar', 'adiantamento', 'cinqüentar', 'função', 'gratificação', 'promoção', 'trabalho', 'ano', 'parcela', 'pagamento', 'empregado', 'pagar', 'férias', 'dia', 'ser', 'empregar', 'empresa', 'salário', 'number']\n",
      "Top 20 words for 'Contract types': ['estagiário', 'mesmo', 'período', 'aprendizagem', 'lei', 'trabalho', 'contratação', 'ter', 'aprendiz', 'estágio', 'função', 'prazo', 'salário', 'dia', 'empregar', 'ser', 'empresa', 'experiência', 'contrato', 'number']\n",
      "Top 20 words for 'Employment protections': ['provisório', 'aposentadoria', 'gestante', 'garantir', 'assegurar', 'período', 'militar', 'salário', 'prazo', 'trabalho', 'ser', 'ano', 'ficar', 'dia', 'empresa', 'emprego', 'estabilidade', 'serviço', 'empregar', 'number']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# creates Pipeline to preprocess and calculate TF-IDF\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"cba_preprocess\", CBAPreprocessLemmatize()),\n",
    "    (\"cba_to_tfidf\", CBAToTFIDFTransformer())\n",
    "])\n",
    "\n",
    "# number of words to output and groupings\n",
    "groups = df.groupby('subgroup')\n",
    "n_top_words = 20\n",
    "\n",
    "for name, group in groups:\n",
    "    data_texts = group['text'].tolist()\n",
    "    data_tfidf = preprocess_pipeline.fit_transform(data_texts)\n",
    "    mean_tfidf = data_tfidf.mean(axis=0)\n",
    "    top_indices = np.argsort(mean_tfidf)[-n_top_words:]\n",
    "    vocab = preprocess_pipeline.named_steps['cba_to_tfidf'].vectorizer.get_feature_names_out()\n",
    "    top_words = [vocab[idx] for idx in top_indices]\n",
    "    print(f\"Top {n_top_words} words for '{name}': {top_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
